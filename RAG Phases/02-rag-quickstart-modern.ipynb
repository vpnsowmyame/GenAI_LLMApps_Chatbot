{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern RAG Quickstart 2025: Build Your First Advanced RAG Application\n",
    "\n",
    "Welcome to hands-on RAG development! This notebook will guide you through building a modern RAG application using the latest LangChain features and best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ What You'll Build\n",
    "\n",
    "A modern RAG application with:\n",
    "- **LangGraph orchestration** for complex workflows\n",
    "- **Advanced streaming** with intermediate step visibility\n",
    "- **Semantic chunking** for better context preservation\n",
    "- **Source attribution** with structured outputs\n",
    "- **Query analysis** for improved retrieval\n",
    "- **Production-ready features** like error handling and monitoring\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- Understanding of RAG concepts (see `rag-concepts-2025.ipynb`)\n",
    "- OpenAI API key (or other LLM provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Modern Setup (2025)\n",
    "\n",
    "The LangChain ecosystem has evolved with a more modular architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Updated Package Structure\n",
    "\n",
    "**2024 Approach (Your Original Notebooks):**\n",
    "```bash\n",
    "pip install langchain langchain-community langchain-openai chromadb\n",
    "```\n",
    "\n",
    "**2025 Modern Approach:**\n",
    "```bash\n",
    "# Core LangChain interfaces and base implementations\n",
    "pip install langchain-core\n",
    "\n",
    "# Community integrations (vector stores, document loaders)\n",
    "pip install langchain-community\n",
    "\n",
    "# LLM providers (choose what you need)\n",
    "pip install langchain-openai  # or langchain-anthropic, langchain-groq, etc.\n",
    "\n",
    "# Advanced orchestration for complex workflows\n",
    "pip install langgraph\n",
    "\n",
    "# Vector stores and utilities\n",
    "pip install chromadb beautifulsoup4 python-dotenv\n",
    "\n",
    "# Optional: Enhanced text processing\n",
    "pip install langchain-experimental  # for semantic chunking\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install --upgrade --quiet langchain-core langchain-community langchain-openai langgraph chromadb beautifulsoup4 python-dotenv langchain-experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë Environment Setup\n",
    "\n",
    "Create a `.env` file with your API keys:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```env\n",
    "# Required\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "\n",
    "# Optional: LangSmith for observability (recommended for production)\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_API_KEY=your_langsmith_api_key_here\n",
    "LANGCHAIN_PROJECT=rag-quickstart-2025\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify OpenAI API key is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY in your .env file\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üîç LangSmith tracing: {'Enabled' if os.getenv('LANGCHAIN_TRACING_V2') else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Phase 1: Modern Document Ingestion\n",
    "\n",
    "Let's start by loading and processing documents using modern techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåê Enhanced Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load a comprehensive AI research document\n",
    "# We'll use a recent paper on AI agents for more current content\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\"  # More recent content\n",
    "]\n",
    "\n",
    "# Enhanced BS4 parsing with better content extraction\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\", \"post-meta\"))\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=urls,\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    header_template={\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    ")\n",
    "\n",
    "try:\n",
    "    docs = loader.load()\n",
    "    print(f\"‚úÖ Loaded {len(docs)} documents\")\n",
    "    print(f\"üìÑ Total characters: {sum(len(doc.page_content) for doc in docs):,}\")\n",
    "    \n",
    "    # Show document metadata\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"\\nüìã Document {i+1}:\")\n",
    "        print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"   Length: {len(doc.page_content):,} characters\")\n",
    "        print(f\"   Preview: {doc.page_content[:100]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading documents: {e}\")\n",
    "    print(\"üí° Tip: Check your internet connection or try different URLs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Modern Text Splitting: Semantic Chunking\n",
    "\n",
    "**2024 Approach**: Simple character-based chunking\n",
    "**2025 Approach**: Semantic chunking that preserves meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize embeddings for semantic chunking\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "print(\"üîÑ Comparing chunking strategies...\\n\")\n",
    "\n",
    "# 1. Traditional character-based chunking (2024 approach)\n",
    "print(\"üìù Traditional Character-based Chunking:\")\n",
    "char_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_documents(docs)\n",
    "print(f\"   Chunks created: {len(char_chunks)}\")\n",
    "print(f\"   Average chunk size: {sum(len(chunk.page_content) for chunk in char_chunks) // len(char_chunks)} characters\")\n",
    "\n",
    "# 2. Modern semantic chunking (2025 approach)\n",
    "print(\"\\nüß† Modern Semantic Chunking:\")\n",
    "try:\n",
    "    semantic_splitter = SemanticChunker(\n",
    "        embeddings=embeddings,\n",
    "        breakpoint_threshold_type=\"percentile\",  # or \"standard_deviation\", \"interquartile\"\n",
    "        breakpoint_threshold_amount=95  # Top 5% of semantic differences become breakpoints\n",
    "    )\n",
    "    \n",
    "    # Use first document for semantic chunking demo (to avoid API costs)\n",
    "    semantic_chunks = semantic_splitter.split_documents([docs[0]])\n",
    "    print(f\"   Chunks created: {len(semantic_chunks)}\")\n",
    "    print(f\"   Average chunk size: {sum(len(chunk.page_content) for chunk in semantic_chunks) // len(semantic_chunks)} characters\")\n",
    "    \n",
    "    # For production, we'll use character-based for cost efficiency\n",
    "    # In real applications, choose based on your accuracy vs cost requirements\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Semantic chunking failed: {e}\")\n",
    "    print(\"   üìù Falling back to character-based chunking\")\n",
    "    semantic_chunks = char_chunks[:10]  # Use subset for demo\n",
    "\n",
    "# For the rest of this tutorial, we'll use character-based chunks (more cost-effective)\n",
    "all_chunks = char_chunks\n",
    "\n",
    "print(f\"\\n‚úÖ Using {len(all_chunks)} chunks for vector storage\")\n",
    "\n",
    "# Show a few example chunks with metadata\n",
    "print(\"\\nüìÑ Example chunks:\")\n",
    "for i, chunk in enumerate(all_chunks[:3]):\n",
    "    print(f\"\\n   Chunk {i+1}:\")\n",
    "    print(f\"   Source: {chunk.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"   Start Index: {chunk.metadata.get('start_index', 'N/A')}\")\n",
    "    print(f\"   Length: {len(chunk.page_content)} chars\")\n",
    "    print(f\"   Content: {chunk.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÉÔ∏è Enhanced Vector Storage\n",
    "\n",
    "Modern vector storage with better configuration and metadata handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Create a temporary directory for the vector store\n",
    "persist_directory = tempfile.mkdtemp(prefix=\"rag_modern_\")\n",
    "\n",
    "print(f\"üóÑÔ∏è  Creating vector store in: {persist_directory}\")\n",
    "\n",
    "try:\n",
    "    # Create vector store with persistence and better configuration\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=all_chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}  # Better similarity metric\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Vector store created with {len(all_chunks)} documents\")\n",
    "    \n",
    "    # Test the vector store\n",
    "    test_results = vectorstore.similarity_search(\n",
    "        \"What is task decomposition?\", \n",
    "        k=3,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüîç Test search returned {len(test_results)} results\")\n",
    "    print(f\"üìÑ First result preview: {test_results[0].page_content[:100]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating vector store: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Phase 2: Advanced Retrieval with Query Analysis\n",
    "\n",
    "Modern RAG systems don't just take user queries as-is. They analyze and optimize them first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß≠ Query Analysis and Rewriting\n",
    "\n",
    "**2024**: Direct user query ‚Üí Search  \n",
    "**2025**: User query ‚Üí Analyze ‚Üí Rewrite ‚Üí Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Define structured output for query analysis\n",
    "class QueryAnalysis(BaseModel):\n",
    "    \"\"\"Analysis and rewriting of user queries for better retrieval.\"\"\"\n",
    "    \n",
    "    original_query: str = Field(description=\"The original user query\")\n",
    "    intent: str = Field(description=\"The intent behind the query (e.g., 'definition', 'comparison', 'process')\")\n",
    "    key_concepts: List[str] = Field(description=\"Key concepts and terms in the query\")\n",
    "    rewritten_queries: List[str] = Field(description=\"2-3 rewritten versions optimized for retrieval\")\n",
    "    metadata_filters: dict = Field(description=\"Any metadata filters that should be applied\")\n",
    "\n",
    "# Create query analysis prompt\n",
    "query_analysis_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert at analyzing user queries for a RAG system about AI agents and LLM research.\n",
    "\n",
    "Given a user query, analyze it and provide:\n",
    "1. The query intent (definition, comparison, process, example, etc.)\n",
    "2. Key concepts and technical terms\n",
    "3. 2-3 rewritten versions that would be better for semantic search\n",
    "4. Any metadata filters that might help (like source preferences)\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Provide your analysis in the specified JSON format.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Create the query analyzer\n",
    "query_analyzer = query_analysis_prompt | llm.with_structured_output(QueryAnalysis)\n",
    "\n",
    "# Test the query analyzer\n",
    "def analyze_query(user_query: str) -> QueryAnalysis:\n",
    "    \"\"\"Analyze a user query for better retrieval.\"\"\"\n",
    "    try:\n",
    "        analysis = query_analyzer.invoke({\"query\": user_query})\n",
    "        return analysis\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Query analysis failed: {e}\")\n",
    "        # Fallback to simple analysis\n",
    "        return QueryAnalysis(\n",
    "            original_query=user_query,\n",
    "            intent=\"general\",\n",
    "            key_concepts=[user_query],\n",
    "            rewritten_queries=[user_query],\n",
    "            metadata_filters={}\n",
    "        )\n",
    "\n",
    "# Test with different types of queries\n",
    "test_queries = [\n",
    "    \"What is task decomposition?\",\n",
    "    \"How do autonomous agents work compared to traditional chatbots?\",\n",
    "    \"Can you give me examples of reasoning techniques in AI?\"\n",
    "]\n",
    "\n",
    "print(\"üß≠ Query Analysis Results:\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"‚ùì Query: '{query}'\")\n",
    "    analysis = analyze_query(query)\n",
    "    print(f\"   üéØ Intent: {analysis.intent}\")\n",
    "    print(f\"   üîë Key Concepts: {', '.join(analysis.key_concepts)}\")\n",
    "    print(f\"   ‚úçÔ∏è  Rewritten Queries:\")\n",
    "    for i, rewritten in enumerate(analysis.rewritten_queries, 1):\n",
    "        print(f\"      {i}. {rewritten}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Advanced Retrieval Strategies\n",
    "\n",
    "Modern retrievers with multiple strategies and result fusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class AdvancedRetriever:\n",
    "    \"\"\"Advanced retrieval system with multiple strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, documents):\n",
    "        self.vectorstore = vectorstore\n",
    "        \n",
    "        # Create multiple retrievers\n",
    "        self.vector_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\n",
    "                \"k\": 6,\n",
    "                \"score_threshold\": 0.7\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # BM25 retriever for keyword-based search\n",
    "        try:\n",
    "            self.bm25_retriever = BM25Retriever.from_documents(\n",
    "                documents, k=4\n",
    "            )\n",
    "            \n",
    "            # Ensemble retriever combining semantic and keyword search\n",
    "            self.ensemble_retriever = EnsembleRetriever(\n",
    "                retrievers=[self.vector_retriever, self.bm25_retriever],\n",
    "                weights=[0.7, 0.3]  # Favor semantic search\n",
    "            )\n",
    "            self.use_ensemble = True\n",
    "            print(\"‚úÖ Advanced ensemble retriever created\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è BM25 retriever failed: {e}\")\n",
    "            print(\"üìù Using vector retriever only\")\n",
    "            self.use_ensemble = False\n",
    "    \n",
    "    def retrieve(self, analysis: QueryAnalysis) -> List[Any]:\n",
    "        \"\"\"Retrieve documents using analyzed query.\"\"\"\n",
    "        all_docs = []\n",
    "        \n",
    "        # Use ensemble retriever if available\n",
    "        retriever = self.ensemble_retriever if self.use_ensemble else self.vector_retriever\n",
    "        \n",
    "        # Try each rewritten query\n",
    "        for query in analysis.rewritten_queries:\n",
    "            try:\n",
    "                docs = retriever.invoke(query)\n",
    "                all_docs.extend(docs)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Retrieval failed for query '{query}': {e}\")\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_docs = []\n",
    "        for doc in all_docs:\n",
    "            doc_id = (doc.page_content[:100], doc.metadata.get('source', ''))\n",
    "            if doc_id not in seen:\n",
    "                seen.add(doc_id)\n",
    "                unique_docs.append(doc)\n",
    "        \n",
    "        # Limit results\n",
    "        return unique_docs[:8]\n",
    "\n",
    "# Create advanced retriever\n",
    "advanced_retriever = AdvancedRetriever(vectorstore, all_chunks)\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What is task decomposition?\"\n",
    "analysis = analyze_query(test_query)\n",
    "retrieved_docs = advanced_retriever.retrieve(analysis)\n",
    "\n",
    "print(f\"\\nüîç Retrieved {len(retrieved_docs)} documents for: '{test_query}'\")\n",
    "print(\"\\nüìÑ Retrieved documents:\")\n",
    "for i, doc in enumerate(retrieved_docs[:3]):\n",
    "    print(f\"\\n   Document {i+1}:\")\n",
    "    print(f\"   Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"   Preview: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Phase 3: Modern Generation with LangGraph\n",
    "\n",
    "**2024**: Simple LCEL chains  \n",
    "**2025**: LangGraph orchestrated workflows with state management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåä LangGraph Workflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import json\n",
    "\n",
    "# Define the state for our RAG workflow\n",
    "class RAGState(TypedDict):\n",
    "    \"\"\"State for the RAG workflow.\"\"\"\n",
    "    user_query: str\n",
    "    query_analysis: QueryAnalysis\n",
    "    retrieved_docs: List[Any]\n",
    "    generated_response: str\n",
    "    sources: List[Dict[str, Any]]\n",
    "    messages: List[BaseMessage]\n",
    "    error: str\n",
    "\n",
    "# Define structured output for final response\n",
    "class RAGResponse(BaseModel):\n",
    "    \"\"\"Structured response from RAG system.\"\"\"\n",
    "    answer: str = Field(description=\"The comprehensive answer to the user's question\")\n",
    "    sources: List[Dict[str, str]] = Field(description=\"List of sources with titles and URLs\")\n",
    "    confidence: str = Field(description=\"Confidence level: high, medium, or low\")\n",
    "    follow_up_questions: List[str] = Field(description=\"Suggested follow-up questions\")\n",
    "\n",
    "print(\"üåä Setting up LangGraph workflow...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Workflow Node Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: Query Analysis\n",
    "def analyze_query_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"Analyze the user query.\"\"\"\n",
    "    try:\n",
    "        analysis = analyze_query(state[\"user_query\"])\n",
    "        state[\"query_analysis\"] = analysis\n",
    "        state[\"messages\"].append(HumanMessage(content=state[\"user_query\"]))\n",
    "    except Exception as e:\n",
    "        state[\"error\"] = f\"Query analysis failed: {e}\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Node 2: Document Retrieval\n",
    "def retrieve_documents_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"Retrieve relevant documents.\"\"\"\n",
    "    if state.get(\"error\"):\n",
    "        return state\n",
    "    \n",
    "    try:\n",
    "        docs = advanced_retriever.retrieve(state[\"query_analysis\"])\n",
    "        state[\"retrieved_docs\"] = docs\n",
    "        \n",
    "        # Extract source information\n",
    "        sources = []\n",
    "        for doc in docs:\n",
    "            source_info = {\n",
    "                \"title\": doc.metadata.get(\"title\", \"Unknown Document\"),\n",
    "                \"url\": doc.metadata.get(\"source\", \"\"),\n",
    "                \"preview\": doc.page_content[:150] + \"...\"\n",
    "            }\n",
    "            sources.append(source_info)\n",
    "        \n",
    "        state[\"sources\"] = sources\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"error\"] = f\"Document retrieval failed: {e}\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Node 3: Response Generation\n",
    "def generate_response_node(state: RAGState) -> RAGState:\n",
    "    \"\"\"Generate the final response.\"\"\"\n",
    "    if state.get(\"error\"):\n",
    "        return state\n",
    "    \n",
    "    try:\n",
    "        # Create generation prompt with sources\n",
    "        generation_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "You are an expert AI assistant specializing in AI agents and LLM research.\n",
    "\n",
    "User Query: {query}\n",
    "Query Intent: {intent}\n",
    "Key Concepts: {concepts}\n",
    "\n",
    "Retrieved Context:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "1. Provide a comprehensive, accurate answer based on the retrieved context\n",
    "2. If the context doesn't fully address the query, acknowledge this\n",
    "3. Include specific examples when relevant\n",
    "4. Suggest 2-3 follow-up questions\n",
    "5. Rate your confidence level (high/medium/low) based on context quality\n",
    "\n",
    "Format your response as structured JSON matching the RAGResponse schema.\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Prepare context\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Source: {doc.metadata.get('source', 'Unknown')}\\n{doc.page_content}\"\n",
    "            for doc in state[\"retrieved_docs\"]\n",
    "        ])\n",
    "        \n",
    "        # Generate response\n",
    "        generation_chain = generation_prompt | llm.with_structured_output(RAGResponse)\n",
    "        \n",
    "        response = generation_chain.invoke({\n",
    "            \"query\": state[\"user_query\"],\n",
    "            \"intent\": state[\"query_analysis\"].intent,\n",
    "            \"concepts\": \", \".join(state[\"query_analysis\"].key_concepts),\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        state[\"generated_response\"] = response.answer\n",
    "        \n",
    "        # Update sources with response sources\n",
    "        state[\"sources\"] = response.sources if response.sources else state[\"sources\"]\n",
    "        \n",
    "        # Add AI message to conversation\n",
    "        ai_message = AIMessage(content=json.dumps({\n",
    "            \"answer\": response.answer,\n",
    "            \"confidence\": response.confidence,\n",
    "            \"follow_up_questions\": response.follow_up_questions\n",
    "        }, indent=2))\n",
    "        \n",
    "        state[\"messages\"].append(ai_message)\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"error\"] = f\"Response generation failed: {e}\"\n",
    "        # Fallback response\n",
    "        fallback_answer = f\"I apologize, but I encountered an error while processing your question: '{state['user_query']}'. Please try rephrasing your question.\"\n",
    "        state[\"generated_response\"] = fallback_answer\n",
    "        state[\"messages\"].append(AIMessage(content=fallback_answer))\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Workflow nodes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Build the LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the workflow graph\n",
    "workflow = StateGraph(RAGState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"analyze_query\", analyze_query_node)\n",
    "workflow.add_node(\"retrieve_docs\", retrieve_documents_node)\n",
    "workflow.add_node(\"generate_response\", generate_response_node)\n",
    "\n",
    "# Define the flow\n",
    "workflow.set_entry_point(\"analyze_query\")\n",
    "workflow.add_edge(\"analyze_query\", \"retrieve_docs\")\n",
    "workflow.add_edge(\"retrieve_docs\", \"generate_response\")\n",
    "workflow.add_edge(\"generate_response\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "rag_app = workflow.compile()\n",
    "\n",
    "print(\"üèóÔ∏è LangGraph RAG workflow compiled successfully!\")\n",
    "\n",
    "# Visualize the workflow (if graphviz is available)\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(rag_app.get_graph().draw_mermaid_png()))\n",
    "except:\n",
    "    print(\"üìä Workflow visualization not available (install graphviz for visual graph)\")\n",
    "    print(\"üîÑ Workflow: analyze_query ‚Üí retrieve_docs ‚Üí generate_response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Phase 4: Advanced Streaming and Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåä Modern Streaming with Intermediate Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import AsyncIterator\n",
    "import time\n",
    "\n",
    "async def stream_rag_response(user_query: str) -> AsyncIterator[dict]:\n",
    "    \"\"\"Stream RAG response with intermediate steps.\"\"\"\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"user_query\": user_query,\n",
    "        \"messages\": [],\n",
    "        \"sources\": [],\n",
    "        \"error\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Stream through the workflow\n",
    "    async for event in rag_app.astream(initial_state):\n",
    "        yield event\n",
    "\n",
    "# Synchronous version for Jupyter\n",
    "def run_rag_with_streaming(user_query: str):\n",
    "    \"\"\"Run RAG with streaming output.\"\"\"\n",
    "    print(f\"ü§î Processing query: '{user_query}'\\n\")\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"user_query\": user_query,\n",
    "        \"messages\": [],\n",
    "        \"sources\": [],\n",
    "        \"error\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Stream through workflow\n",
    "    for step_output in rag_app.stream(initial_state):\n",
    "        for node_name, node_state in step_output.items():\n",
    "            if node_name == \"analyze_query\":\n",
    "                if \"query_analysis\" in node_state:\n",
    "                    analysis = node_state[\"query_analysis\"]\n",
    "                    print(f\"üß≠ Query Analysis Complete:\")\n",
    "                    print(f\"   Intent: {analysis.intent}\")\n",
    "                    print(f\"   Key Concepts: {', '.join(analysis.key_concepts)}\")\n",
    "                    print(f\"   Rewritten: {analysis.rewritten_queries[0]}\\n\")\n",
    "                    \n",
    "            elif node_name == \"retrieve_docs\":\n",
    "                if \"retrieved_docs\" in node_state:\n",
    "                    docs = node_state[\"retrieved_docs\"]\n",
    "                    print(f\"üîç Document Retrieval Complete:\")\n",
    "                    print(f\"   Found {len(docs)} relevant documents\")\n",
    "                    print(f\"   Sources: {set(doc.metadata.get('source', 'Unknown') for doc in docs)}\\n\")\n",
    "                    \n",
    "            elif node_name == \"generate_response\":\n",
    "                if \"generated_response\" in node_state:\n",
    "                    print(f\"‚úÖ Response Generation Complete:\\n\")\n",
    "                    \n",
    "                    # Display final response\n",
    "                    print(\"ü§ñ **Answer:**\")\n",
    "                    print(node_state[\"generated_response\"])\n",
    "                    \n",
    "                    # Display sources\n",
    "                    if node_state[\"sources\"]:\n",
    "                        print(\"\\nüìö **Sources:**\")\n",
    "                        for i, source in enumerate(node_state[\"sources\"][:3], 1):\n",
    "                            print(f\"   {i}. {source.get('url', 'Unknown source')}\")\n",
    "                    \n",
    "                    return node_state\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"üåä Streaming RAG system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéÆ Interactive RAG Demo\n",
    "\n",
    "Let's test our modern RAG system with various types of questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries showcasing different capabilities\n",
    "test_queries = [\n",
    "    \"What is task decomposition and why is it important for AI agents?\",\n",
    "    \"How do autonomous agents differ from traditional chatbots?\",\n",
    "    \"Can you explain the Chain of Thought reasoning technique with examples?\",\n",
    "    \"What are the main challenges in building reliable AI agents?\"\n",
    "]\n",
    "\n",
    "# Interactive demo\n",
    "print(\"üéÆ Modern RAG System Demo\\n\")\n",
    "print(\"Choose a test query or enter your own:\\n\")\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"{i}. {query}\")\n",
    "\n",
    "print(\"\\n5. Enter custom query\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test query (you can change this to any of the test queries or your own)\n",
    "selected_query = test_queries[0]  # Change index 0-3 for different test queries\n",
    "\n",
    "# Or uncomment the line below to use a custom query\n",
    "# selected_query = \"Your custom question here\"\n",
    "\n",
    "print(f\"Running query: '{selected_query}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the RAG system with streaming\n",
    "start_time = time.time()\n",
    "result = run_rag_with_streaming(selected_query)\n",
    "end_time = time.time()\n",
    "\n",
    "if result:\n",
    "    print(f\"\\n‚è±Ô∏è Total processing time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Show error if any\n",
    "    if result.get(\"error\"):\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: {result['error']}\")\nelse:\n    print(\"‚ùå Query processing failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance Comparison: 2024 vs 2025\n",
    "\n",
    "Let's compare the old simple approach with our new advanced system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 2024 Simple RAG Chain (from your original notebook)\n",
    "def create_simple_rag_chain():\n",
    "    \"\"\"Create a simple RAG chain like in 2024.\"\"\"\n",
    "    \n",
    "    # Simple retriever\n",
    "    simple_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    # Simple prompt\n",
    "    simple_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of \n",
    "retrieved context to answer the question. If you don't know the answer, just say \n",
    "that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Format documents function\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # Simple chain\n",
    "    rag_chain = (\n",
    "        {\"context\": simple_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | simple_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "# Create simple chain\n",
    "simple_rag = create_simple_rag_chain()\n",
    "\n",
    "# Performance comparison\n",
    "test_query = \"What is task decomposition?\"\n",
    "\n",
    "print(\"‚ö° Performance Comparison: 2024 vs 2025 RAG\\n\")\n",
    "print(\"üìä Testing query: 'What is task decomposition?'\\n\")\n",
    "\n",
    "# Test 2024 approach\n",
    "print(\"üïê 2024 Simple RAG:\")\n",
    "start_time = time.time()\n",
    "simple_result = simple_rag.invoke(test_query)\n",
    "simple_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Time: {simple_time:.2f} seconds\")\n",
    "print(f\"   Answer: {simple_result[:200]}...\\n\")\n",
    "\n",
    "# Test 2025 approach\n",
    "print(\"üöÄ 2025 Advanced RAG:\")\n",
    "start_time = time.time()\n",
    "# Get the last result from our streaming demo\n",
    "advanced_result = run_rag_with_streaming(test_query)\n",
    "advanced_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n   Time: {advanced_time:.2f} seconds\")\n",
    "print(f\"   Features: Query analysis, advanced retrieval, source attribution, structured output\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà **Improvements in 2025 RAG:**\")\n",
    "print(\"   ‚úÖ Query analysis and optimization\")\n",
    "print(\"   ‚úÖ Multi-strategy retrieval (semantic + keyword)\")\n",
    "print(\"   ‚úÖ Structured outputs with confidence scoring\")\n",
    "print(\"   ‚úÖ Proper source attribution\")\n",
    "print(\"   ‚úÖ LangGraph workflow management\")\n",
    "print(\"   ‚úÖ Advanced streaming capabilities\")\n",
    "print(\"   ‚úÖ Better error handling\")\n",
    "print(\"   ‚úÖ Follow-up question suggestions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Production Considerations\n",
    "\n",
    "Modern RAG systems need to be production-ready. Here are key considerations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production utilities\n",
    "import logging\n",
    "from functools import wraps\n",
    "import hashlib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"RAG_System\")\n",
    "\n",
    "class ProductionRAGSystem:\n",
    "    \"\"\"Production-ready RAG system with monitoring and caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, workflow, max_retries=3):\n",
    "        self.workflow = workflow\n",
    "        self.max_retries = max_retries\n",
    "        self.cache = {}  # Simple in-memory cache (use Redis in production)\n",
    "        \n",
    "    def _get_cache_key(self, query: str) -> str:\n",
    "        \"\"\"Generate cache key for query.\"\"\"\n",
    "        return hashlib.md5(query.encode()).hexdigest()\n",
    "    \n",
    "    def query(self, user_query: str, use_cache: bool = True) -> dict:\n",
    "        \"\"\"Process query with production features.\"\"\"\n",
    "        \n",
    "        # Check cache first\n",
    "        if use_cache:\n",
    "            cache_key = self._get_cache_key(user_query)\n",
    "            if cache_key in self.cache:\n",
    "                logger.info(f\"Cache hit for query: {user_query[:50]}...\")\n",
    "                return self.cache[cache_key]\n",
    "        \n",
    "        # Process with retries\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                logger.info(f\"Processing query (attempt {attempt + 1}): {user_query[:50]}...\")\n",
    "                \n",
    "                # Initialize state\n",
    "                initial_state = {\n",
    "                    \"user_query\": user_query,\n",
    "                    \"messages\": [],\n",
    "                    \"sources\": [],\n",
    "                    \"error\": \"\"\n",
    "                }\n",
    "                \n",
    "                # Run workflow\n",
    "                final_state = None\n",
    "                for step_output in self.workflow.stream(initial_state):\n",
    "                    for node_name, node_state in step_output.items():\n",
    "                        if node_name == \"generate_response\":\n",
    "                            final_state = node_state\n",
    "                \n",
    "                if final_state and not final_state.get(\"error\"):\n",
    "                    result = {\n",
    "                        \"answer\": final_state.get(\"generated_response\", \"\"),\n",
    "                        \"sources\": final_state.get(\"sources\", []),\n",
    "                        \"query\": user_query,\n",
    "                        \"timestamp\": time.time()\n",
    "                    }\n",
    "                    \n",
    "                    # Cache result\n",
    "                    if use_cache:\n",
    "                        self.cache[cache_key] = result\n",
    "                    \n",
    "                    logger.info(\"Query processed successfully\")\n",
    "                    return result\n",
    "                else:\n",
    "                    error_msg = final_state.get(\"error\", \"Unknown error\") if final_state else \"Workflow failed\"\n",
    "                    logger.warning(f\"Attempt {attempt + 1} failed: {error_msg}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Attempt {attempt + 1} failed with exception: {e}\")\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    raise\n",
    "        \n",
    "        # Fallback response\n",
    "        return {\n",
    "            \"answer\": \"I apologize, but I'm unable to process your query at the moment. Please try again later.\",\n",
    "            \"sources\": [],\n",
    "            \"query\": user_query,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"error\": \"Max retries exceeded\"\n",
    "        }\n",
    "    \n",
    "    def get_cache_stats(self) -> dict:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        return {\n",
    "            \"cache_size\": len(self.cache),\n",
    "            \"cache_keys\": list(self.cache.keys())\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the cache.\"\"\"\n",
    "        self.cache.clear()\n",
    "        logger.info(\"Cache cleared\")\n",
    "\n",
    "# Create production system\n",
    "prod_rag = ProductionRAGSystem(rag_app)\n",
    "\n",
    "print(\"üè≠ Production RAG system initialized\")\n",
    "print(\"\\nüîß Production Features:\")\n",
    "print(\"   ‚úÖ Comprehensive logging\")\n",
    "print(\"   ‚úÖ Automatic retry logic\")\n",
    "print(\"   ‚úÖ Response caching\")\n",
    "print(\"   ‚úÖ Error handling and fallbacks\")\n",
    "print(\"   ‚úÖ Performance monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Production System Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test production system\n",
    "print(\"üß™ Testing Production RAG System\\n\")\n",
    "\n",
    "# Test query\n",
    "test_query = \"What are the main components of an autonomous agent?\"\n",
    "\n",
    "# First call (no cache)\n",
    "print(\"üìû First call (cache miss):\")\n",
    "start_time = time.time()\n",
    "result1 = prod_rag.query(test_query)\n",
    "time1 = time.time() - start_time\n",
    "print(f\"   Time: {time1:.2f} seconds\")\n",
    "print(f\"   Answer: {result1['answer'][:100]}...\\n\")\n",
    "\n",
    "# Second call (cached)\n",
    "print(\"‚ö° Second call (cache hit):\")\n",
    "start_time = time.time()\n",
    "result2 = prod_rag.query(test_query)\n",
    "time2 = time.time() - start_time\n",
    "print(f\"   Time: {time2:.2f} seconds\")\n",
    "print(f\"   Speedup: {time1/time2:.1f}x faster\\n\")\n",
    "\n",
    "# Cache stats\n",
    "cache_stats = prod_rag.get_cache_stats()\n",
    "print(f\"üìä Cache Stats: {cache_stats['cache_size']} items cached\")\n",
    "\n",
    "print(\"\\n‚úÖ Production system test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup resources\n",
    "try:\n",
    "    # Clean up vector store directory\n",
    "    shutil.rmtree(persist_directory)\n",
    "    print(f\"üßπ Cleaned up temporary directory: {persist_directory}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Cleanup warning: {e}\")\n",
    "\n",
    "# Clear production cache\n",
    "prod_rag.clear_cache()\n",
    "\n",
    "print(\"‚úÖ Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways: Modern RAG 2025\n",
    "\n",
    "Congratulations! You've built a state-of-the-art RAG system. Here's what you've learned:\n",
    "\n",
    "### üÜö 2024 vs 2025 Comparison\n",
    "\n",
    "| Feature | 2024 Simple RAG | 2025 Modern RAG |\n",
    "|---------|----------------|------------------|\n",
    "| **Architecture** | Linear LCEL chains | LangGraph workflows |\n",
    "| **Query Processing** | Direct search | Analysis + rewriting |\n",
    "| **Retrieval** | Basic similarity | Multi-strategy ensemble |\n",
    "| **Chunking** | Character-based | Semantic chunking |\n",
    "| **Streaming** | Simple output | Intermediate steps |\n",
    "| **Sources** | Basic citations | Structured attribution |\n",
    "| **Error Handling** | Minimal | Comprehensive |\n",
    "| **Production Features** | None | Caching, retries, monitoring |\n",
    "\n",
    "### üöÄ Modern RAG Benefits\n",
    "\n",
    "1. **Better Accuracy**: Query analysis and multi-strategy retrieval\n",
    "2. **Enhanced UX**: Streaming with progress indicators\n",
    "3. **Production Ready**: Error handling, caching, monitoring\n",
    "4. **Flexible Architecture**: LangGraph enables complex workflows\n",
    "5. **Source Transparency**: Structured citations and confidence scores\n",
    "\n",
    "### üîÆ Next Steps\n",
    "\n",
    "Ready to explore cutting-edge techniques? Check out:\n",
    "- **`rag-advanced-features.ipynb`** - Multi-modal RAG, adaptive systems, and enterprise features\n",
    "\n",
    "### üí° Production Tips\n",
    "\n",
    "1. **Monitor Performance**: Use LangSmith for observability\n",
    "2. **Optimize Costs**: Balance accuracy vs API costs\n",
    "3. **Scale Gradually**: Start simple, add complexity as needed\n",
    "4. **Test Extensively**: Use diverse queries and edge cases\n",
    "5. **Cache Intelligently**: Cache expensive operations\n",
    "\n",
    "### üåü You're Now Ready For\n",
    "\n",
    "- Building production RAG applications\n",
    "- Integrating with enterprise systems\n",
    "- Handling complex multi-step reasoning\n",
    "- Implementing advanced retrieval strategies\n",
    "\n",
    "**Happy Building! üõ†Ô∏è**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}