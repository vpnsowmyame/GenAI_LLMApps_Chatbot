{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational RAG 2025: Building Chat-Enabled RAG Systems\n",
    "\n",
    "Learn how to build RAG systems that maintain conversation context and provide natural chat experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ What You'll Learn\n",
    "\n",
    "- **Chat History Management** - Maintaining context across multiple turns\n",
    "- **Memory Systems** - Different approaches to conversation memory\n",
    "- **Context Compression** - Handling long conversations efficiently\n",
    "- **Follow-up Questions** - Generating relevant follow-up suggestions\n",
    "- **Session Management** - User-specific conversation handling\n",
    "- **Advanced Streaming** - Real-time conversational responses\n",
    "\n",
    "## ðŸ“‹ Prerequisites\n",
    "\n",
    "- Completed `rag-quickstart-modern.ipynb`\n",
    "- Understanding of basic RAG concepts\n",
    "- OpenAI API key or other LLM provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages for conversational RAG\n",
    "%pip install --upgrade --quiet langchain-core langchain-community langchain-openai langgraph chromadb python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Core LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# LangGraph for conversation orchestration\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "import bs4\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY in your .env file\")\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Quick Knowledge Base Setup\n",
    "\n",
    "Let's create a knowledge base for our conversational RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\"\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=urls,\n",
    "    bs_kwargs={\"parse_only\": bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))}\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"ðŸ“„ Loaded {len(docs)} documents\")\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(f\"ðŸ“ Created {len(splits)} chunks\")\n",
    "\n",
    "# Create vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=tempfile.mkdtemp(prefix=\"conv_rag_\")\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "print(\"ðŸ—„ï¸ Vector store ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¬ Understanding Conversational RAG\n",
    "\n",
    "### ðŸ†š Single-Turn vs Multi-Turn RAG\n",
    "\n",
    "**Single-Turn RAG (Basic):**\n",
    "```\n",
    "User: \"What is task decomposition?\"\n",
    "RAG: [Searches] â†’ [Generates response]\n",
    "```\n",
    "\n",
    "**Multi-Turn RAG (Conversational):**\n",
    "```\n",
    "User: \"What is task decomposition?\"\n",
    "RAG: [Searches + Responds] â†’ \"Task decomposition breaks complex tasks...\"\n",
    "\n",
    "User: \"Can you give me some examples?\"\n",
    "RAG: [Remembers context] â†’ [Searches for examples] â†’ [Responds with examples]\n",
    "\n",
    "User: \"How does this compare to chain-of-thought?\"\n",
    "RAG: [Understands \"this\" = task decomposition] â†’ [Searches] â†’ [Compares both concepts]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§  Memory Strategies\n",
    "\n",
    "**1. Full History** - Keep all messages (simple but expensive)  \n",
    "**2. Sliding Window** - Keep last N messages  \n",
    "**3. Summary Memory** - Summarize older conversations  \n",
    "**4. Entity Memory** - Track important entities across conversation  \n",
    "**5. Compressed Memory** - Compress context while preserving key information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Building Conversational RAG Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“‹ Conversational State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational RAG State\n",
    "class ConversationalRAGState(TypedDict):\n",
    "    \"\"\"State for conversational RAG workflow.\"\"\"\n",
    "    messages: List[BaseMessage]  # Full conversation history\n",
    "    user_query: str  # Current user question\n",
    "    reformulated_query: str  # Context-aware query for retrieval\n",
    "    retrieved_docs: List[Any]  # Retrieved documents\n",
    "    response: str  # Generated response\n",
    "    sources: List[Dict[str, Any]]  # Source information\n",
    "    follow_ups: List[str]  # Suggested follow-up questions\n",
    "    session_id: str  # Session identifier\n",
    "    error: Optional[str]  # Error message if any\n",
    "\n",
    "# Response structure\n",
    "class ConversationalResponse(BaseModel):\n",
    "    \"\"\"Structured response for conversational RAG.\"\"\"\n",
    "    answer: str = Field(description=\"The response to the user's question\")\n",
    "    confidence: str = Field(description=\"Confidence level: high, medium, low\")\n",
    "    sources: List[Dict[str, str]] = Field(description=\"Sources used for the answer\")\n",
    "    follow_up_questions: List[str] = Field(description=\"Suggested follow-up questions\")\n",
    "    context_used: bool = Field(description=\"Whether conversation history was used\")\n",
    "\n",
    "print(\"ðŸ“‹ Conversational state defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”„ Query Reformulation for Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Query reformulation for conversational context\n",
    "reformulation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are an expert at reformulating user questions in conversational contexts.\n",
    "\n",
    "Given a conversation history and a new user question, reformulate the question to:\n",
    "1. Be self-contained (include necessary context from conversation)\n",
    "2. Resolve pronouns and references (\"it\", \"this\", \"that\", etc.)\n",
    "3. Be optimized for semantic search in a knowledge base\n",
    "\n",
    "If the question is already self-contained, return it as-is.\n",
    "\n",
    "Examples:\n",
    "History: [\"What is task decomposition?\", \"Task decomposition breaks complex tasks into smaller parts...\"]\n",
    "Question: \"Can you give me examples?\"\n",
    "Reformulated: \"Can you give me examples of task decomposition techniques?\"\n",
    "\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"conversation_history\"),\n",
    "    (\"human\", \"New question: {question}\\n\\nReformulated question:\")\n",
    "])\n",
    "\n",
    "reformulation_chain = reformulation_prompt | llm\n",
    "\n",
    "def reformulate_query(messages: List[BaseMessage], current_query: str) -> str:\n",
    "    \"\"\"Reformulate query based on conversation context.\"\"\"\n",
    "    try:\n",
    "        # Only use recent history (last 6 messages) to avoid token limits\n",
    "        recent_history = messages[-6:] if len(messages) > 6 else messages\n",
    "        \n",
    "        result = reformulation_chain.invoke({\n",
    "            \"conversation_history\": recent_history,\n",
    "            \"question\": current_query\n",
    "        })\n",
    "        \n",
    "        reformulated = result.content.strip()\n",
    "        \n",
    "        # Fallback to original if reformulation seems problematic\n",
    "        if len(reformulated) < len(current_query) * 0.5 or len(reformulated) > len(current_query) * 3:\n",
    "            return current_query\n",
    "        \n",
    "        return reformulated\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Query reformulation failed: {e}\")\n",
    "        return current_query\n",
    "\n",
    "# Test reformulation\n",
    "test_history = [\n",
    "    HumanMessage(content=\"What is task decomposition?\"),\n",
    "    AIMessage(content=\"Task decomposition is a technique that breaks down complex tasks into smaller, manageable steps...\")\n",
    "]\n",
    "\n",
    "test_query = \"Can you give me some examples?\"\n",
    "reformulated = reformulate_query(test_history, test_query)\n",
    "\n",
    "print(\"ðŸ§ª Query Reformulation Test:\")\n",
    "print(f\"   Original: '{test_query}'\")\n",
    "print(f\"   Reformulated: '{reformulated}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– Conversational Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational response generation\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are an expert AI assistant specializing in AI agents and LLM research.\n",
    "You're having a natural conversation with a user about these topics.\n",
    "\n",
    "Guidelines:\n",
    "1. Use the retrieved context to provide accurate, detailed answers\n",
    "2. Reference the conversation history when relevant\n",
    "3. Be conversational and engaging, not robotic\n",
    "4. If the context doesn't fully address the question, acknowledge this\n",
    "5. Suggest relevant follow-up questions to continue the conversation\n",
    "6. Use examples and analogies to make complex concepts clear\n",
    "\n",
    "Retrieved Context:\n",
    "{context}\n",
    "\n",
    "Answer the user's question in a conversational manner and provide follow-up suggestions.\n",
    "\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"conversation_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Create conversational chain with structured output\n",
    "conversational_chain = conversational_prompt | llm.with_structured_output(ConversationalResponse)\n",
    "\n",
    "def generate_conversational_response(\n",
    "    question: str,\n",
    "    context_docs: List[Any],\n",
    "    conversation_history: List[BaseMessage]\n",
    ") -> ConversationalResponse:\n",
    "    \"\"\"Generate a conversational response with context.\"\"\"\n",
    "    \n",
    "    # Format context\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Source: {doc.metadata.get('source', 'Unknown')}\\n{doc.page_content}\"\n",
    "        for doc in context_docs\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # Use recent conversation history (last 8 messages)\n",
    "        recent_history = conversation_history[-8:] if len(conversation_history) > 8 else conversation_history\n",
    "        \n",
    "        response = conversational_chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"conversation_history\": recent_history\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Response generation failed: {e}\")\n",
    "        # Fallback response\n",
    "        return ConversationalResponse(\n",
    "            answer=\"I apologize, but I encountered an error generating a response. Could you please rephrase your question?\",\n",
    "            confidence=\"low\",\n",
    "            sources=[],\n",
    "            follow_up_questions=[\"Could you rephrase your question?\", \"Is there a specific aspect you'd like to know about?\"],\n",
    "            context_used=len(conversation_history) > 0\n",
    "        )\n",
    "\n",
    "print(\"ðŸ¤– Conversational response generator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒŠ LangGraph Conversational Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow nodes\n",
    "def reformulate_query_node(state: ConversationalRAGState) -> ConversationalRAGState:\n",
    "    \"\"\"Reformulate query based on conversation context.\"\"\"\n",
    "    try:\n",
    "        reformulated = reformulate_query(state[\"messages\"], state[\"user_query\"])\n",
    "        state[\"reformulated_query\"] = reformulated\n",
    "        \n",
    "        print(f\"ðŸ”„ Query reformulated: '{state['user_query']}' â†’ '{reformulated}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"error\"] = f\"Query reformulation failed: {e}\"\n",
    "        state[\"reformulated_query\"] = state[\"user_query\"]\n",
    "    \n",
    "    return state\n",
    "\n",
    "def retrieve_documents_node(state: ConversationalRAGState) -> ConversationalRAGState:\n",
    "    \"\"\"Retrieve documents based on reformulated query.\"\"\"\n",
    "    if state.get(\"error\"):\n",
    "        return state\n",
    "    \n",
    "    try:\n",
    "        docs = retriever.invoke(state[\"reformulated_query\"])\n",
    "        state[\"retrieved_docs\"] = docs\n",
    "        \n",
    "        # Extract sources\n",
    "        sources = []\n",
    "        for doc in docs:\n",
    "            sources.append({\n",
    "                \"title\": \"AI Research Document\",\n",
    "                \"url\": doc.metadata.get(\"source\", \"\"),\n",
    "                \"snippet\": doc.page_content[:150] + \"...\"\n",
    "            })\n",
    "        \n",
    "        state[\"sources\"] = sources\n",
    "        print(f\"ðŸ” Retrieved {len(docs)} documents\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"error\"] = f\"Document retrieval failed: {e}\"\n",
    "        state[\"retrieved_docs\"] = []\n",
    "        state[\"sources\"] = []\n",
    "    \n",
    "    return state\n",
    "\n",
    "def generate_response_node(state: ConversationalRAGState) -> ConversationalRAGState:\n",
    "    \"\"\"Generate conversational response.\"\"\"\n",
    "    if state.get(\"error\"):\n",
    "        return state\n",
    "    \n",
    "    try:\n",
    "        # Generate response\n",
    "        response = generate_conversational_response(\n",
    "            state[\"user_query\"],\n",
    "            state[\"retrieved_docs\"],\n",
    "            state[\"messages\"]\n",
    "        )\n",
    "        \n",
    "        state[\"response\"] = response.answer\n",
    "        state[\"follow_ups\"] = response.follow_up_questions\n",
    "        \n",
    "        # Add messages to conversation history\n",
    "        state[\"messages\"].append(HumanMessage(content=state[\"user_query\"]))\n",
    "        state[\"messages\"].append(AIMessage(content=response.answer))\n",
    "        \n",
    "        print(f\"âœ… Generated conversational response\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"error\"] = f\"Response generation failed: {e}\"\n",
    "        fallback_response = \"I apologize, but I'm having trouble processing your question right now.\"\n",
    "        state[\"response\"] = fallback_response\n",
    "        state[\"follow_ups\"] = [\"Could you try rephrasing your question?\"]\n",
    "        \n",
    "        # Still add to history\n",
    "        state[\"messages\"].append(HumanMessage(content=state[\"user_query\"]))\n",
    "        state[\"messages\"].append(AIMessage(content=fallback_response))\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Build the conversational workflow\n",
    "conv_workflow = StateGraph(ConversationalRAGState)\n",
    "\n",
    "# Add nodes\n",
    "conv_workflow.add_node(\"reformulate\", reformulate_query_node)\n",
    "conv_workflow.add_node(\"retrieve\", retrieve_documents_node)\n",
    "conv_workflow.add_node(\"generate\", generate_response_node)\n",
    "\n",
    "# Define flow\n",
    "conv_workflow.set_entry_point(\"reformulate\")\n",
    "conv_workflow.add_edge(\"reformulate\", \"retrieve\")\n",
    "conv_workflow.add_edge(\"retrieve\", \"generate\")\n",
    "conv_workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Add memory for persistence\n",
    "memory = MemorySaver()\n",
    "conv_app = conv_workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"ðŸŒŠ Conversational RAG workflow compiled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¬ Conversational RAG Session Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalRAGSession:\n",
    "    \"\"\"Manages a conversational RAG session with memory and context.\"\"\"\n",
    "    \n",
    "    def __init__(self, workflow, session_id: str = None):\n",
    "        self.workflow = workflow\n",
    "        self.session_id = session_id or str(uuid.uuid4())\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def ask(self, question: str, stream: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"Ask a question in the conversational context.\"\"\"\n",
    "        \n",
    "        # Prepare initial state\n",
    "        config = {\"configurable\": {\"thread_id\": self.session_id}}\n",
    "        \n",
    "        initial_state = {\n",
    "            \"messages\": self.conversation_history.copy(),\n",
    "            \"user_query\": question,\n",
    "            \"session_id\": self.session_id,\n",
    "            \"sources\": [],\n",
    "            \"follow_ups\": [],\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nðŸ¤” User: {question}\")\n",
    "        \n",
    "        try:\n",
    "            result = self.workflow.invoke(initial_state, config)\n",
    "            \n",
    "            # Update conversation history\n",
    "            self.conversation_history = result[\"messages\"]\n",
    "            \n",
    "            return {\n",
    "                \"response\": result[\"response\"],\n",
    "                \"sources\": result[\"sources\"],\n",
    "                \"follow_ups\": result[\"follow_ups\"],\n",
    "                \"session_id\": self.session_id,\n",
    "                \"error\": result.get(\"error\")\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Session processing failed: {e}\"}\n",
    "    \n",
    "    def get_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get conversation history in a readable format.\"\"\"\n",
    "        history = []\n",
    "        for msg in self.conversation_history:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                history.append({\"role\": \"user\", \"content\": msg.content})\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                history.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "        return history\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"ðŸ§¹ Conversation history cleared\")\n",
    "    \n",
    "    def get_session_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get session information.\"\"\"\n",
    "        return {\n",
    "            \"session_id\": self.session_id,\n",
    "            \"message_count\": len(self.conversation_history),\n",
    "            \"conversation_turns\": len(self.conversation_history) // 2\n",
    "        }\n",
    "\n",
    "print(\"ðŸ’¬ Conversational RAG Session Manager ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ® Interactive Conversational Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversational session\n",
    "chat_session = ConversationalRAGSession(conv_app)\n",
    "\n",
    "print(f\"ðŸŽ® Conversational RAG Demo Started\")\n",
    "print(f\"ðŸ“ Session ID: {chat_session.session_id}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo conversation 1\n",
    "result1 = chat_session.ask(\"What is task decomposition?\")\n",
    "\n",
    "if \"error\" not in result1:\n",
    "    print(f\"\\nðŸ¤– Assistant: {result1['response']}\")\n",
    "    \n",
    "    if result1[\"follow_ups\"]:\n",
    "        print(\"\\nðŸ’¡ Suggested follow-up questions:\")\n",
    "        for i, follow_up in enumerate(result1[\"follow_ups\"], 1):\n",
    "            print(f\"   {i}. {follow_up}\")\nelse:\n    print(f\"âŒ Error: {result1['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo conversation 2 - Follow-up question\n",
    "result2 = chat_session.ask(\"Can you give me some examples?\")\n",
    "\n",
    "if \"error\" not in result2:\n",
    "    print(f\"\\nðŸ¤– Assistant: {result2['response']}\")\n",
    "    \n",
    "    if result2[\"follow_ups\"]:\n",
    "        print(\"\\nðŸ’¡ Suggested follow-up questions:\")\n",
    "        for i, follow_up in enumerate(result2[\"follow_ups\"], 1):\n",
    "            print(f\"   {i}. {follow_up}\")\nelse:\n    print(f\"âŒ Error: {result2['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Conversation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the conversation\n",
    "session_info = chat_session.get_session_info()\n",
    "history = chat_session.get_history()\n",
    "\n",
    "print(\"ðŸ“Š Conversation Analysis:\")\n",
    "print(f\"   Session ID: {session_info['session_id']}\")\n",
    "print(f\"   Total Messages: {session_info['message_count']}\")\n",
    "print(f\"   Conversation Turns: {session_info['conversation_turns']}\")\n",
    "\n",
    "print(\"\\nðŸ” Full Conversation History:\")\n",
    "for i, exchange in enumerate(history, 1):\n",
    "    role = \"ðŸ‘¤ User\" if exchange[\"role\"] == \"user\" else \"ðŸ¤– Assistant\"\n",
    "    content = exchange[\"content\"]\n",
    "    if len(content) > 100:\n",
    "        content = content[:100] + \"...\"\n",
    "    print(f\"   {i}. {role}: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Takeaways: Conversational RAG\n",
    "\n",
    "### âœ… What You've Built\n",
    "\n",
    "1. **Query Reformulation** - Context-aware query processing\n",
    "2. **Memory Management** - Multiple strategies for conversation history\n",
    "3. **Session Management** - Multi-user conversation handling\n",
    "4. **Production Features** - Error handling, cleanup, monitoring\n",
    "\n",
    "### ðŸ§  Memory Strategy Guide\n",
    "\n",
    "| Strategy | Best For | Pros | Cons |\n",
    "|----------|----------|------|------|\n",
    "| **Sliding Window** | Short conversations, cost-conscious | Fast, predictable cost | Loses early context |\n",
    "| **Summary Memory** | Long conversations, context preservation | Retains key information | More complex, LLM calls for summaries |\n",
    "| **Full History** | Critical applications, short sessions | Complete context | Expensive for long conversations |\n",
    "\n",
    "### ðŸš€ Production Recommendations\n",
    "\n",
    "1. **Start Simple** - Use sliding window for most applications\n",
    "2. **Monitor Usage** - Track conversation length and costs\n",
    "3. **Session Cleanup** - Implement automatic session expiration\n",
    "4. **Error Handling** - Graceful degradation when memory operations fail\n",
    "5. **User Experience** - Provide follow-up suggestions to guide conversations\n",
    "\n",
    "### ðŸ”® Next Steps\n",
    "\n",
    "- **Multi-modal RAG** - Handle images, documents, and rich media\n",
    "- **Production Deployment** - Scalability, monitoring, and enterprise features\n",
    "\n",
    "You now have a production-ready conversational RAG system! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}