{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-modal RAG 2025: Beyond Text - Images, PDFs, Audio & Video\n",
    "\n",
    "Explore how to build RAG systems that work with images, documents, audio, and video content using modern LangChain capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ What You'll Learn\n",
    "\n",
    "- **Multi-modal Understanding** - Processing different content types\n",
    "- **Image RAG** - Extract and search visual information\n",
    "- **Document RAG** - Advanced PDF processing with layout awareness\n",
    "- **Audio/Video RAG** - Transcription and content extraction\n",
    "- **Unified Search** - Cross-modal retrieval and generation\n",
    "- **Vision-Language Models** - Modern multimodal AI capabilities\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Completed previous RAG notebooks\n",
    "- OpenAI API key (for vision models)\n",
    "- Additional API keys for specialized services (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Enhanced Setup for Multi-modal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages for multi-modal RAG\n",
    "%pip install --upgrade --quiet langchain-core langchain-community langchain-openai langgraph chromadb python-dotenv\n",
    "%pip install --upgrade --quiet unstructured[all-docs] pillow pytesseract pdf2image\n",
    "%pip install --upgrade --quiet opencv-python-headless whisper-openai python-magic-bin  # python-magic for Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Core LangChain\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Multi-modal processing\n",
    "from langchain_community.document_loaders import (\n",
    "    UnstructuredPDFLoader,\n",
    "    UnstructuredImageLoader,\n",
    "    UnstructuredFileLoader\n",
    ")\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY in your .env file\")\n",
    "\n",
    "print(\"‚úÖ Multi-modal setup complete!\")\n",
    "print(\"üé® Image processing: PIL, OpenCV\")\n",
    "print(\"üìÑ Document processing: Unstructured\")\n",
    "print(\"üß† Vision model: GPT-4 Vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Image RAG: Visual Content Processing\n",
    "\n",
    "Let's start with processing and understanding images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì∏ Image Analysis and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vision-capable model\n",
    "vision_llm = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1000)\n",
    "\n",
    "def encode_image_to_base64(image_path_or_url: str) -> str:\n",
    "    \"\"\"Convert image to base64 for API consumption.\"\"\"\n",
    "    if image_path_or_url.startswith(('http://', 'https://')):\n",
    "        # URL image\n",
    "        response = requests.get(image_path_or_url)\n",
    "        return base64.b64encode(response.content).decode('utf-8')\n",
    "    else:\n",
    "        # Local file\n",
    "        with open(image_path_or_url, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "class ImageAnalyzer:\n",
    "    \"\"\"Analyze images using vision models.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm=None):\n",
    "        self.llm = llm or vision_llm\n",
    "        \n",
    "        # Analysis prompt\n",
    "        self.analysis_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"\n",
    "You are an expert at analyzing images and extracting detailed information.\n",
    "Analyze the provided image and describe:\n",
    "\n",
    "1. **Main Subject**: What is the primary focus of the image?\n",
    "2. **Visual Elements**: Colors, composition, style, objects present\n",
    "3. **Text Content**: Any text visible in the image (OCR)\n",
    "4. **Context & Setting**: Where/when might this be from?\n",
    "5. **Technical Details**: Image quality, type, notable features\n",
    "6. **Searchable Keywords**: Key terms for semantic search\n",
    "\n",
    "Provide a comprehensive description that would be useful for:\n",
    "- Semantic search and retrieval\n",
    "- Answering questions about the image content\n",
    "- Understanding the visual context\n",
    "\"\"\"),\n",
    "            (\"human\", [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please analyze this image in detail:\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"data:image/jpeg;base64,{image_data}\"\n",
    "                    }\n",
    "                }\n",
    "            ])\n",
    "        ])\n",
    "    \n",
    "    def analyze_image(self, image_path_or_url: str, metadata: Dict[str, Any] = None) -> Document:\n",
    "        \"\"\"Analyze an image and return a document with analysis.\"\"\"\n",
    "        try:\n",
    "            # Encode image\n",
    "            image_base64 = encode_image_to_base64(image_path_or_url)\n",
    "            \n",
    "            # Create the message with the image\n",
    "            messages = self.analysis_prompt.format_messages(image_data=image_base64)\n",
    "            \n",
    "            # Get analysis\n",
    "            response = self.llm.invoke(messages)\n",
    "            \n",
    "            # Create document\n",
    "            doc_metadata = {\n",
    "                \"source\": image_path_or_url,\n",
    "                \"type\": \"image\",\n",
    "                \"analysis_model\": self.llm.model_name,\n",
    "                **(metadata or {})\n",
    "            }\n",
    "            \n",
    "            return Document(\n",
    "                page_content=response.content,\n",
    "                metadata=doc_metadata\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Image analysis failed: {e}\")\n",
    "            return Document(\n",
    "                page_content=f\"Failed to analyze image: {str(e)}\",\n",
    "                metadata={\"source\": image_path_or_url, \"type\": \"image\", \"error\": str(e)}\n",
    "            )\n",
    "    \n",
    "    def analyze_multiple_images(self, image_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"Analyze multiple images.\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for i, image_path in enumerate(image_paths):\n",
    "            print(f\"üñºÔ∏è Analyzing image {i+1}/{len(image_paths)}: {Path(image_path).name}\")\n",
    "            doc = self.analyze_image(image_path, {\"image_index\": i})\n",
    "            documents.append(doc)\n",
    "        \n",
    "        return documents\n",
    "\n",
    "# Create image analyzer\n",
    "image_analyzer = ImageAnalyzer()\n",
    "print(\"üì∏ Image analyzer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test with some sample images (using URLs for demo)\n",
    "sample_images = [\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/280px-PNG_transparency_demonstration_1.png\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Vd-Orig.svg/256px-Vd-Orig.svg.png\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing image analysis with sample images...\")\n",
    "\n",
    "try:\n",
    "    # Analyze first image\n",
    "    test_doc = image_analyzer.analyze_image(sample_images[0])\n",
    "    print(f\"\\n‚úÖ Image Analysis Result:\")\n",
    "    print(f\"Source: {test_doc.metadata['source']}\")\n",
    "    print(f\"Analysis: {test_doc.page_content[:300]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Image analysis test failed: {e}\")\n",
    "    print(\"üí° This might be due to API limitations or network issues.\")\n",
    "    print(\"üí° In production, ensure you have access to vision models.\")\n",
    "    \n",
    "    # Create mock document for demonstration\n",
    "    test_doc = Document(\n",
    "        page_content=\"This is a demonstration image showing PNG transparency with a checkered pattern background. The image contains geometric shapes and demonstrates alpha channel transparency capabilities.\",\n",
    "        metadata={\"source\": sample_images[0], \"type\": \"image\", \"mock\": True}\n",
    "    )\n",
    "    print(\"\\nüîÑ Using mock analysis for demo purposes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Advanced Document Processing\n",
    "\n",
    "Beyond simple text extraction - understand document structure, tables, and visual elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è Intelligent PDF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedDocumentProcessor:\n",
    "    \"\"\"Advanced document processing with layout awareness.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.supported_types = {\n",
    "            '.pdf': self._process_pdf,\n",
    "            '.docx': self._process_docx,\n",
    "            '.pptx': self._process_pptx,\n",
    "            '.txt': self._process_text,\n",
    "            '.md': self._process_text\n",
    "        }\n",
    "    \n",
    "    def process_document(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Process a document with structure awareness.\"\"\"\n",
    "        file_path = Path(file_path)\n",
    "        extension = file_path.suffix.lower()\n",
    "        \n",
    "        if extension not in self.supported_types:\n",
    "            raise ValueError(f\"Unsupported file type: {extension}\")\n",
    "        \n",
    "        processor = self.supported_types[extension]\n",
    "        return processor(str(file_path))\n",
    "    \n",
    "    def _process_pdf(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Process PDF with advanced structure extraction.\"\"\"\n",
    "        try:\n",
    "            # Use Unstructured for advanced PDF processing\n",
    "            loader = UnstructuredPDFLoader(\n",
    "                file_path,\n",
    "                mode=\"elements\",  # Extract individual elements\n",
    "                strategy=\"hi_res\"  # High resolution processing\n",
    "            )\n",
    "            \n",
    "            elements = loader.load()\n",
    "            \n",
    "            # Group elements by type and enhance metadata\n",
    "            documents = []\n",
    "            for element in elements:\n",
    "                # Enhance metadata with element information\n",
    "                enhanced_metadata = {\n",
    "                    \"source\": file_path,\n",
    "                    \"type\": \"pdf\",\n",
    "                    \"element_type\": element.metadata.get(\"category\", \"unknown\"),\n",
    "                    \"page_number\": element.metadata.get(\"page_number\", 1),\n",
    "                    **element.metadata\n",
    "                }\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=element.page_content,\n",
    "                    metadata=enhanced_metadata\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            \n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Advanced PDF processing failed: {e}\")\n",
    "            # Fallback to simple processing\n",
    "            return self._simple_pdf_fallback(file_path)\n",
    "    \n",
    "    def _simple_pdf_fallback(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Simple PDF fallback processing.\"\"\"\n",
    "        try:\n",
    "            loader = UnstructuredPDFLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            \n",
    "            # Add basic metadata\n",
    "            for doc in docs:\n",
    "                doc.metadata.update({\n",
    "                    \"type\": \"pdf\",\n",
    "                    \"processing\": \"fallback\"\n",
    "                })\n",
    "            \n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå PDF processing completely failed: {e}\")\n",
    "            return [Document(\n",
    "                page_content=f\"Failed to process PDF: {file_path}\",\n",
    "                metadata={\"source\": file_path, \"type\": \"pdf\", \"error\": str(e)}\n",
    "            )]\n",
    "    \n",
    "    def _process_docx(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Process Word documents.\"\"\"\n",
    "        try:\n",
    "            loader = UnstructuredFileLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            \n",
    "            for doc in docs:\n",
    "                doc.metadata.update({\"type\": \"docx\"})\n",
    "            \n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            return [Document(\n",
    "                page_content=f\"Failed to process DOCX: {str(e)}\",\n",
    "                metadata={\"source\": file_path, \"type\": \"docx\", \"error\": str(e)}\n",
    "            )]\n",
    "    \n",
    "    def _process_pptx(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Process PowerPoint presentations.\"\"\"\n",
    "        try:\n",
    "            loader = UnstructuredFileLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            \n",
    "            for doc in docs:\n",
    "                doc.metadata.update({\"type\": \"pptx\"})\n",
    "            \n",
    "            return docs\n",
    "        except Exception as e:\n",
    "            return [Document(\n",
    "                page_content=f\"Failed to process PPTX: {str(e)}\",\n",
    "                metadata={\"source\": file_path, \"type\": \"pptx\", \"error\": str(e)}\n",
    "            )]\n",
    "    \n",
    "    def _process_text(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Process plain text files.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            return [Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"source\": file_path,\n",
    "                    \"type\": Path(file_path).suffix[1:],  # Remove the dot\n",
    "                    \"length\": len(content)\n",
    "                }\n",
    "            )]\n",
    "        except Exception as e:\n",
    "            return [Document(\n",
    "                page_content=f\"Failed to process text file: {str(e)}\",\n",
    "                metadata={\"source\": file_path, \"type\": \"text\", \"error\": str(e)}\n",
    "            )]\n",
    "\n",
    "# Create document processor\n",
    "doc_processor = AdvancedDocumentProcessor()\n",
    "print(\"üìÑ Advanced document processor ready\")\n",
    "print(f\"   Supported types: {list(doc_processor.supported_types.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéß Audio and Video Processing\n",
    "\n",
    "Extract and process content from audio and video files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioVideoProcessor:\n",
    "    \"\"\"Process audio and video files for RAG.\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key: str = None):\n",
    "        self.openai_api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    def process_audio(self, audio_path: str, metadata: Dict[str, Any] = None) -> Document:\n",
    "        \"\"\"Process audio file using Whisper API.\"\"\"\n",
    "        try:\n",
    "            import openai\n",
    "            \n",
    "            # Initialize OpenAI client\n",
    "            client = openai.OpenAI(api_key=self.openai_api_key)\n",
    "            \n",
    "            # Transcribe audio\n",
    "            with open(audio_path, \"rb\") as audio_file:\n",
    "                transcript = client.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\", \n",
    "                    file=audio_file,\n",
    "                    response_format=\"verbose_json\"\n",
    "                )\n",
    "            \n",
    "            # Create document with enhanced metadata\n",
    "            doc_metadata = {\n",
    "                \"source\": audio_path,\n",
    "                \"type\": \"audio\",\n",
    "                \"duration\": transcript.duration if hasattr(transcript, 'duration') else None,\n",
    "                \"language\": transcript.language if hasattr(transcript, 'language') else None,\n",
    "                \"transcription_model\": \"whisper-1\",\n",
    "                **(metadata or {})\n",
    "            }\n",
    "            \n",
    "            return Document(\n",
    "                page_content=transcript.text,\n",
    "                metadata=doc_metadata\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Audio processing failed: {e}\")\n",
    "            return Document(\n",
    "                page_content=f\"Failed to process audio: {str(e)}\",\n",
    "                metadata={\"source\": audio_path, \"type\": \"audio\", \"error\": str(e)}\n",
    "            )\n",
    "    \n",
    "    def process_video(self, video_path: str, extract_audio: bool = True, extract_frames: bool = False) -> List[Document]:\n",
    "        \"\"\"Process video file - extract audio transcription and optionally analyze frames.\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        if extract_audio:\n",
    "            # Extract audio and transcribe\n",
    "            try:\n",
    "                # Use ffmpeg to extract audio (this is a simplified example)\n",
    "                import subprocess\n",
    "                \n",
    "                audio_path = video_path.replace('.mp4', '_audio.wav').replace('.mov', '_audio.wav')\n",
    "                \n",
    "                # Extract audio using ffmpeg (requires ffmpeg installation)\n",
    "                subprocess.run([\n",
    "                    'ffmpeg', '-i', video_path, '-q:a', '0', '-map', 'a', audio_path, '-y'\n",
    "                ], check=True, capture_output=True)\n",
    "                \n",
    "                # Process the extracted audio\n",
    "                audio_doc = self.process_audio(audio_path, {\"extracted_from_video\": True})\n",
    "                documents.append(audio_doc)\n",
    "                \n",
    "                # Clean up temporary audio file\n",
    "                os.remove(audio_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Video audio extraction failed: {e}\")\n",
    "                documents.append(Document(\n",
    "                    page_content=f\"Failed to extract audio from video: {str(e)}\",\n",
    "                    metadata={\"source\": video_path, \"type\": \"video_audio\", \"error\": str(e)}\n",
    "                ))\n",
    "        \n",
    "        if extract_frames:\n",
    "            # Extract and analyze key frames\n",
    "            try:\n",
    "                frame_docs = self._extract_and_analyze_frames(video_path)\n",
    "                documents.extend(frame_docs)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Video frame extraction failed: {e}\")\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def _extract_and_analyze_frames(self, video_path: str, num_frames: int = 5) -> List[Document]:\n",
    "        \"\"\"Extract key frames from video and analyze them.\"\"\"\n",
    "        try:\n",
    "            # Use OpenCV to extract frames\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            \n",
    "            frame_documents = []\n",
    "            \n",
    "            # Extract frames at regular intervals\n",
    "            for i in range(num_frames):\n",
    "                frame_number = int((i + 1) * total_frames / (num_frames + 1))\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "                \n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    # Save frame temporarily\n",
    "                    temp_frame_path = f\"temp_frame_{i}.jpg\"\n",
    "                    cv2.imwrite(temp_frame_path, frame)\n",
    "                    \n",
    "                    # Analyze frame with vision model\n",
    "                    frame_doc = image_analyzer.analyze_image(\n",
    "                        temp_frame_path,\n",
    "                        {\n",
    "                            \"source_video\": video_path,\n",
    "                            \"frame_number\": frame_number,\n",
    "                            \"frame_index\": i\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    frame_documents.append(frame_doc)\n",
    "                    \n",
    "                    # Clean up temporary frame\n",
    "                    os.remove(temp_frame_path)\n",
    "            \n",
    "            cap.release()\n",
    "            return frame_documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Frame extraction failed: {e}\")\n",
    "            return []\n",
    "\n",
    "# Create audio/video processor\n",
    "av_processor = AudioVideoProcessor()\n",
    "print(\"üéß Audio/Video processor ready\")\n",
    "print(\"   üìù Supports: Audio transcription, Video frame extraction\")\n",
    "print(\"   ‚ö†Ô∏è  Note: Requires ffmpeg for video processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Unified Multi-modal RAG System\n",
    "\n",
    "Combine all content types into a unified RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "class MultimodalRAGSystem:\n",
    "    \"\"\"Unified RAG system handling multiple content types.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1000)\n",
    "        \n",
    "        # Processors\n",
    "        self.image_analyzer = ImageAnalyzer()\n",
    "        self.doc_processor = AdvancedDocumentProcessor()\n",
    "        self.av_processor = AudioVideoProcessor()\n",
    "        \n",
    "        # Text splitter for long documents\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            add_start_index=True\n",
    "        )\n",
    "        \n",
    "        # Vector store\n",
    "        self.vectorstore = None\n",
    "        self.documents = []\n",
    "    \n",
    "    def add_content(self, content_path: str, content_type: str = None) -> int:\n",
    "        \"\"\"Add content of various types to the knowledge base.\"\"\"\n",
    "        content_path = Path(content_path)\n",
    "        \n",
    "        if content_type is None:\n",
    "            # Auto-detect content type\n",
    "            content_type = self._detect_content_type(content_path)\n",
    "        \n",
    "        print(f\"üîÑ Processing {content_type} content: {content_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            if content_type == \"image\":\n",
    "                docs = [self.image_analyzer.analyze_image(str(content_path))]\n",
    "            elif content_type == \"document\":\n",
    "                docs = self.doc_processor.process_document(str(content_path))\n",
    "            elif content_type == \"audio\":\n",
    "                docs = [self.av_processor.process_audio(str(content_path))]\n",
    "            elif content_type == \"video\":\n",
    "                docs = self.av_processor.process_video(str(content_path))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "            \n",
    "            # Split long documents\n",
    "            split_docs = []\n",
    "            for doc in docs:\n",
    "                if len(doc.page_content) > 1000:\n",
    "                    chunks = self.text_splitter.split_documents([doc])\n",
    "                    split_docs.extend(chunks)\n",
    "                else:\n",
    "                    split_docs.append(doc)\n",
    "            \n",
    "            self.documents.extend(split_docs)\n",
    "            print(f\"‚úÖ Added {len(split_docs)} document chunks\")\n",
    "            \n",
    "            return len(split_docs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to process {content_path}: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def _detect_content_type(self, content_path: Path) -> str:\n",
    "        \"\"\"Auto-detect content type from file extension.\"\"\"\n",
    "        extension = content_path.suffix.lower()\n",
    "        \n",
    "        image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp'}\n",
    "        document_extensions = {'.pdf', '.docx', '.pptx', '.txt', '.md'}\n",
    "        audio_extensions = {'.mp3', '.wav', '.m4a', '.flac', '.ogg'}\n",
    "        video_extensions = {'.mp4', '.mov', '.avi', '.mkv', '.webm'}\n",
    "        \n",
    "        if extension in image_extensions:\n",
    "            return \"image\"\n",
    "        elif extension in document_extensions:\n",
    "            return \"document\"\n",
    "        elif extension in audio_extensions:\n",
    "            return \"audio\"\n",
    "        elif extension in video_extensions:\n",
    "            return \"video\"\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build the vector index from all documents.\"\"\"\n",
    "        if not self.documents:\n",
    "            raise ValueError(\"No documents to index. Add content first.\")\n",
    "        \n",
    "        print(f\"üèóÔ∏è Building index from {len(self.documents)} documents...\")\n",
    "        \n",
    "        # Create vector store\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=tempfile.mkdtemp(prefix=\"multimodal_rag_\")\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Multimodal index built successfully\")\n",
    "    \n",
    "    def search(self, query: str, k: int = 5, filter_by_type: str = None) -> List[Document]:\n",
    "        \"\"\"Search across all content types.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"Index not built. Call build_index() first.\")\n",
    "        \n",
    "        search_kwargs = {\"k\": k}\n",
    "        \n",
    "        # Add type filter if specified\n",
    "        if filter_by_type:\n",
    "            search_kwargs[\"filter\"] = {\"type\": filter_by_type}\n",
    "        \n",
    "        return self.vectorstore.similarity_search(query, **search_kwargs)\n",
    "    \n",
    "    def query(self, question: str, include_sources: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Query the multimodal RAG system.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"Index not built. Call build_index() first.\")\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = self.search(question)\n",
    "        \n",
    "        # Prepare context\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        \n",
    "        for doc in relevant_docs:\n",
    "            content_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "            source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "            \n",
    "            context_parts.append(\n",
    "                f\"[{content_type.upper()} CONTENT from {Path(source).name}]\\n{doc.page_content}\"\n",
    "            )\n",
    "            \n",
    "            if include_sources:\n",
    "                sources.append({\n",
    "                    \"type\": content_type,\n",
    "                    \"source\": source,\n",
    "                    \"preview\": doc.page_content[:150] + \"...\"\n",
    "                })\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate response\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "You are an expert assistant with access to multimodal content including text, images, audio, and video.\n",
    "\n",
    "Answer the user's question based on the provided context from various content types.\n",
    "Be specific about which type of content your answer comes from when relevant.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt.format(context=context, question=question))\n",
    "            \n",
    "            return {\n",
    "                \"answer\": response.content,\n",
    "                \"sources\": sources if include_sources else [],\n",
    "                \"context_types\": list(set(doc.metadata.get(\"type\", \"unknown\") for doc in relevant_docs))\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Failed to generate response: {e}\",\n",
    "                \"sources\": sources if include_sources else []\n",
    "            }\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the knowledge base.\"\"\"\n",
    "        if not self.documents:\n",
    "            return {\"total_documents\": 0}\n",
    "        \n",
    "        type_counts = {}\n",
    "        total_content_length = 0\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            content_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "            type_counts[content_type] = type_counts.get(content_type, 0) + 1\n",
    "            total_content_length += len(doc.page_content)\n",
    "        \n",
    "        return {\n",
    "            \"total_documents\": len(self.documents),\n",
    "            \"content_types\": type_counts,\n",
    "            \"total_content_length\": total_content_length,\n",
    "            \"indexed\": self.vectorstore is not None\n",
    "        }\n",
    "\n",
    "# Create multimodal RAG system\n",
    "multimodal_rag = MultimodalRAGSystem()\n",
    "print(\"üåê Multimodal RAG system initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Multimodal RAG Demo\n",
    "\n",
    "Let's demonstrate the system with different content types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Add sample content (using mock data for demonstration)\n",
    "print(\"üéÆ Multimodal RAG Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Add sample text document\n",
    "sample_text = \"\"\"\n",
    "# AI Agents Architecture\n",
    "\n",
    "Modern AI agents consist of several key components:\n",
    "\n",
    "## Core Components\n",
    "1. **Reasoning Engine**: Handles logical inference and decision-making\n",
    "2. **Memory System**: Stores both short-term and long-term information\n",
    "3. **Tool Interface**: Allows interaction with external systems\n",
    "4. **Planning Module**: Breaks down complex tasks into manageable steps\n",
    "\n",
    "## Task Decomposition Methods\n",
    "- Chain of Thought (CoT): Sequential reasoning steps\n",
    "- Tree of Thoughts (ToT): Exploring multiple reasoning paths\n",
    "- Hierarchical Task Networks (HTN): Multi-level planning\n",
    "\n",
    "These components work together to create intelligent, autonomous systems capable of complex problem-solving.\n",
    "\"\"\"\n",
    "\n",
    "# Create a temporary text file\n",
    "temp_text_file = \"temp_ai_agents.md\"\n",
    "with open(temp_text_file, \"w\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Add to multimodal RAG\n",
    "try:\n",
    "    added_docs = multimodal_rag.add_content(temp_text_file, \"document\")\n",
    "    print(f\"üìù Added text document: {added_docs} chunks\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Text processing failed: {e}\")\n",
    "\n",
    "# Add sample image analysis (mock)\n",
    "try:\n",
    "    # Create a mock image document since we might not have actual image processing\n",
    "    mock_image_doc = Document(\n",
    "        page_content=\"\"\"This image shows a diagram of an AI agent architecture. \n",
    "        The diagram contains several connected boxes representing different components:\n",
    "        - A central 'Reasoning Engine' in blue\n",
    "        - Connected modules for 'Memory', 'Planning', and 'Tools'\n",
    "        - Arrows showing data flow between components\n",
    "        - The overall layout suggests a modular, interconnected system\n",
    "        - Text labels are in Arial font, with a clean technical diagram style\n",
    "        - Color scheme uses blues and grays for a professional appearance\"\"\",\n",
    "        metadata={\"source\": \"ai_agent_diagram.png\", \"type\": \"image\", \"mock\": True}\n",
    "    )\n",
    "    multimodal_rag.documents.append(mock_image_doc)\n",
    "    print(\"üñºÔ∏è Added image analysis (mock)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Image processing failed: {e}\")\n",
    "\n",
    "# Add mock audio transcription\n",
    "try:\n",
    "    mock_audio_doc = Document(\n",
    "        page_content=\"\"\"Welcome to this presentation on AI agents. In this session, we'll explore \n",
    "        the fundamental components that make up modern artificial intelligence agents. \n",
    "        First, let's discuss the reasoning engine, which serves as the brain of the agent. \n",
    "        The reasoning engine processes information, makes decisions, and determines the best \n",
    "        course of action. Next, we have the memory system, which stores both immediate context \n",
    "        and long-term knowledge. This is crucial for maintaining coherent conversations and \n",
    "        learning from past experiences. The planning module is responsible for breaking down \n",
    "        complex tasks into manageable steps, using techniques like hierarchical task networks.\"\"\",\n",
    "        metadata={\"source\": \"ai_agents_lecture.mp3\", \"type\": \"audio\", \"mock\": True, \"duration\": 180}\n",
    "    )\n",
    "    multimodal_rag.documents.append(mock_audio_doc)\n",
    "    print(\"üéß Added audio transcription (mock)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Audio processing failed: {e}\")\n",
    "\n",
    "# Show statistics\n",
    "stats = multimodal_rag.get_statistics()\n",
    "print(f\"\\nüìä Knowledge Base Statistics:\")\n",
    "print(f\"   Total Documents: {stats['total_documents']}\")\n",
    "print(f\"   Content Types: {stats['content_types']}\")\n",
    "print(f\"   Total Content Length: {stats['total_content_length']:,} characters\")\n",
    "\n",
    "# Clean up\n",
    "os.remove(temp_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the multimodal index\n",
    "print(\"üèóÔ∏è Building multimodal search index...\")\n",
    "try:\n",
    "    multimodal_rag.build_index()\n",
    "    print(\"‚úÖ Index built successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Index building failed: {e}\")\n",
    "    print(\"üí° This might be due to lack of documents or API issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multimodal queries\n",
    "if multimodal_rag.vectorstore:\n",
    "    print(\"üîç Testing Multimodal Queries\\n\")\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What are the main components of an AI agent?\",\n",
    "        \"Can you describe what the diagram shows?\",\n",
    "        \"What did the speaker say about memory systems?\",\n",
    "        \"How do reasoning engines work according to the available content?\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"‚ùì Query {i}: {query}\")\n",
    "        \n",
    "        try:\n",
    "            result = multimodal_rag.query(query)\n",
    "            \n",
    "            if \"error\" in result:\n",
    "                print(f\"   ‚ùå Error: {result['error']}\")\n",
    "            else:\n",
    "                print(f\"   ü§ñ Answer: {result['answer'][:200]}...\")\n",
    "                print(f\"   üìö Sources: {result['context_types']}\")\n",
    "                \n",
    "                if result['sources']:\n",
    "                    print(f\"   üîó Source Details:\")\n",
    "                    for source in result['sources'][:2]:  # Show first 2 sources\n",
    "                        print(f\"      - {source['type']}: {source['source']}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Query failed: {e}\")\n",
    "        \n",
    "        print()\nelse:\n    print(\"‚ö†Ô∏è Skipping queries - index not built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Advanced Multimodal Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Cross-Modal Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalRetriever:\n",
    "    \"\"\"Advanced retriever for cross-modal queries.\"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "    \n",
    "    def retrieve_cross_modal(self, query: str, k: int = 8) -> Dict[str, List[Document]]:\n",
    "        \"\"\"Retrieve documents across different modalities.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return {\"error\": \"Vector store not initialized\"}\n",
    "        \n",
    "        # Get all relevant documents\n",
    "        all_docs = self.vectorstore.similarity_search(query, k=k*2)\n",
    "        \n",
    "        # Group by content type\n",
    "        grouped_docs = {\n",
    "            \"text\": [],\n",
    "            \"image\": [],\n",
    "            \"audio\": [],\n",
    "            \"video\": [],\n",
    "            \"document\": []\n",
    "        }\n",
    "        \n",
    "        for doc in all_docs:\n",
    "            content_type = doc.metadata.get(\"type\", \"document\")\n",
    "            if content_type in grouped_docs:\n",
    "                grouped_docs[content_type].append(doc)\n",
    "            else:\n",
    "                grouped_docs[\"document\"].append(doc)\n",
    "        \n",
    "        # Balance the results across modalities\n",
    "        balanced_results = {}\n",
    "        for content_type, docs in grouped_docs.items():\n",
    "            if docs:\n",
    "                balanced_results[content_type] = docs[:k//len([k for k, v in grouped_docs.items() if v])]\n",
    "        \n",
    "        return balanced_results\n",
    "    \n",
    "    def synthesize_cross_modal_response(self, query: str, grouped_docs: Dict[str, List[Document]]) -> str:\n",
    "        \"\"\"Generate response synthesizing information from multiple modalities.\"\"\"\n",
    "        \n",
    "        # Prepare context from different modalities\n",
    "        context_parts = []\n",
    "        \n",
    "        for content_type, docs in grouped_docs.items():\n",
    "            if docs:\n",
    "                type_content = f\"\\n=== {content_type.upper()} SOURCES ===\\n\"\n",
    "                for doc in docs:\n",
    "                    type_content += f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\"\n",
    "                    type_content += f\"Content: {doc.page_content[:300]}...\\n\\n\"\n",
    "                \n",
    "                context_parts.append(type_content)\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate synthesized response\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "You are an expert at synthesizing information from multiple types of content.\n",
    "\n",
    "Given content from text documents, images, audio, and video sources, provide a comprehensive \n",
    "answer that draws insights from all available modalities.\n",
    "\n",
    "Specifically mention which types of content contributed to your answer and how they complement each other.\n",
    "\n",
    "Available Content:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Comprehensive Answer:\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt.format(context=context, query=query))\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating cross-modal response: {e}\"\n",
    "\n",
    "# Create cross-modal retriever\n",
    "if multimodal_rag.vectorstore:\n",
    "    cross_modal_retriever = CrossModalRetriever(multimodal_rag.vectorstore, multimodal_rag.llm)\n",
    "    print(\"üéØ Cross-modal retriever initialized\")\n",
    "    \n",
    "    # Test cross-modal retrieval\n",
    "    test_query = \"Explain AI agent architecture using all available information\"\n",
    "    \n",
    "    print(f\"\\nüîç Cross-Modal Query: {test_query}\")\n",
    "    \n",
    "    grouped_results = cross_modal_retriever.retrieve_cross_modal(test_query)\n",
    "    \n",
    "    print(\"üìä Cross-Modal Results:\")\n",
    "    for content_type, docs in grouped_results.items():\n",
    "        if docs:\n",
    "            print(f\"   {content_type}: {len(docs)} documents\")\n",
    "    \n",
    "    # Generate synthesized response\n",
    "    synthesized_response = cross_modal_retriever.synthesize_cross_modal_response(test_query, grouped_results)\n",
    "    print(f\"\\nüß† Synthesized Response:\")\n",
    "    print(f\"{synthesized_response[:500]}...\")\nelse:\n    print(\"‚ö†Ô∏è Cross-modal retriever not available - vector store not built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Production Considerations for Multimodal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionMultimodalRAG:\n",
    "    \"\"\"Production-ready multimodal RAG with optimizations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.content_cache = {}  # Cache processed content\n",
    "        self.processing_stats = {\n",
    "            \"images_processed\": 0,\n",
    "            \"documents_processed\": 0,\n",
    "            \"audio_processed\": 0,\n",
    "            \"video_processed\": 0,\n",
    "            \"total_processing_time\": 0\n",
    "        }\n",
    "        \n",
    "        # Cost tracking (approximate)\n",
    "        self.cost_estimates = {\n",
    "            \"vision_api_calls\": 0,\n",
    "            \"whisper_api_calls\": 0,\n",
    "            \"embedding_tokens\": 0\n",
    "        }\n",
    "    \n",
    "    def process_content_batch(self, content_paths: List[str], batch_size: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Process multiple content items efficiently.\"\"\"\n",
    "        import time\n",
    "        \n",
    "        results = {\n",
    "            \"processed\": 0,\n",
    "            \"failed\": 0,\n",
    "            \"skipped_cache\": 0,\n",
    "            \"processing_time\": 0\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process in batches to avoid overwhelming APIs\n",
    "        for i in range(0, len(content_paths), batch_size):\n",
    "            batch = content_paths[i:i+batch_size]\n",
    "            \n",
    "            print(f\"üîÑ Processing batch {i//batch_size + 1}/{(len(content_paths) + batch_size - 1)//batch_size}\")\n",
    "            \n",
    "            for content_path in batch:\n",
    "                try:\n",
    "                    # Check cache first\n",
    "                    cache_key = str(content_path)\n",
    "                    if cache_key in self.content_cache:\n",
    "                        results[\"skipped_cache\"] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Process content\n",
    "                    content_type = self._detect_content_type(Path(content_path))\n",
    "                    \n",
    "                    if content_type == \"image\":\n",
    "                        self.cost_estimates[\"vision_api_calls\"] += 1\n",
    "                        self.processing_stats[\"images_processed\"] += 1\n",
    "                    elif content_type == \"audio\":\n",
    "                        self.cost_estimates[\"whisper_api_calls\"] += 1\n",
    "                        self.processing_stats[\"audio_processed\"] += 1\n",
    "                    \n",
    "                    # Cache result (in production, use persistent cache)\n",
    "                    self.content_cache[cache_key] = {\n",
    "                        \"processed_at\": time.time(),\n",
    "                        \"content_type\": content_type\n",
    "                    }\n",
    "                    \n",
    "                    results[\"processed\"] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Failed to process {content_path}: {e}\")\n",
    "                    results[\"failed\"] += 1\n",
    "            \n",
    "            # Add delay between batches to respect rate limits\n",
    "            if i + batch_size < len(content_paths):\n",
    "                time.sleep(1)  # 1 second delay\n",
    "        \n",
    "        results[\"processing_time\"] = time.time() - start_time\n",
    "        self.processing_stats[\"total_processing_time\"] += results[\"processing_time\"]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _detect_content_type(self, content_path: Path) -> str:\n",
    "        \"\"\"Detect content type from extension.\"\"\"\n",
    "        extension = content_path.suffix.lower()\n",
    "        \n",
    "        type_mappings = {\n",
    "            'image': {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp'},\n",
    "            'document': {'.pdf', '.docx', '.pptx', '.txt', '.md'},\n",
    "            'audio': {'.mp3', '.wav', '.m4a', '.flac', '.ogg'},\n",
    "            'video': {'.mp4', '.mov', '.avi', '.mkv', '.webm'}\n",
    "        }\n",
    "        \n",
    "        for content_type, extensions in type_mappings.items():\n",
    "            if extension in extensions:\n",
    "                return content_type\n",
    "        \n",
    "        return 'unknown'\n",
    "    \n",
    "    def estimate_costs(self) -> Dict[str, float]:\n",
    "        \"\"\"Estimate API costs (approximate).\"\"\"\n",
    "        # Rough cost estimates (as of 2025)\n",
    "        cost_per_call = {\n",
    "            \"vision_api_calls\": 0.01,  # $0.01 per image\n",
    "            \"whisper_api_calls\": 0.006,  # $0.006 per minute\n",
    "            \"embedding_tokens\": 0.0001 / 1000  # $0.0001 per 1K tokens\n",
    "        }\n",
    "        \n",
    "        estimated_costs = {}\n",
    "        total_cost = 0\n",
    "        \n",
    "        for service, count in self.cost_estimates.items():\n",
    "            cost = count * cost_per_call[service]\n",
    "            estimated_costs[service] = cost\n",
    "            total_cost += cost\n",
    "        \n",
    "        estimated_costs[\"total\"] = total_cost\n",
    "        return estimated_costs\n",
    "    \n",
    "    def get_performance_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive performance report.\"\"\"\n",
    "        return {\n",
    "            \"processing_stats\": self.processing_stats,\n",
    "            \"cost_estimates\": self.estimate_costs(),\n",
    "            \"cache_stats\": {\n",
    "                \"items_cached\": len(self.content_cache),\n",
    "                \"cache_hit_ratio\": len([v for v in self.content_cache.values() if v]) / max(1, len(self.content_cache))\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create production system\n",
    "prod_multimodal = ProductionMultimodalRAG()\n",
    "\n",
    "# Demo with mock data\n",
    "mock_content_paths = [\n",
    "    \"image1.jpg\", \"image2.png\", \"document1.pdf\", \n",
    "    \"audio1.mp3\", \"video1.mp4\"\n",
    "]\n",
    "\n",
    "print(\"üè≠ Production Multimodal RAG System\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test batch processing\n",
    "batch_results = prod_multimodal.process_content_batch(mock_content_paths)\n",
    "print(f\"üìä Batch Processing Results:\")\nfor key, value in batch_results.items():\n    print(f\"   {key}: {value}\")\n\n# Get performance report\nreport = prod_multimodal.get_performance_report()\nprint(f\"\\nüìà Performance Report:\")\nprint(f\"   Processing Stats: {report['processing_stats']}\")\nprint(f\"   Estimated Costs: ${report['cost_estimates']['total']:.4f}\")\nprint(f\"   Cache Stats: {report['cache_stats']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways: Multimodal RAG\n",
    "\n",
    "### ‚úÖ What You've Built\n",
    "\n",
    "1. **Image Analysis System** - Extract and understand visual content\n",
    "2. **Advanced Document Processing** - Structure-aware PDF/document handling\n",
    "3. **Audio/Video Processing** - Transcription and frame analysis\n",
    "4. **Unified Multimodal Search** - Cross-modal retrieval and synthesis\n",
    "5. **Production Optimizations** - Batch processing, caching, cost estimation\n",
    "\n",
    "### üß† Multimodal RAG Best Practices\n",
    "\n",
    "**Content Processing:**\n",
    "- Use high-resolution processing for documents when layout matters\n",
    "- Extract key frames from videos rather than processing every frame\n",
    "- Cache expensive operations (vision API, transcription)\n",
    "- Implement fallback processing for when advanced methods fail\n",
    "\n",
    "**Search & Retrieval:**\n",
    "- Balance results across different modalities\n",
    "- Use content-type aware metadata for filtering\n",
    "- Implement cross-modal understanding (text query ‚Üí image results)\n",
    "\n",
    "**Cost Management:**\n",
    "- Monitor API usage across vision, transcription, and embedding services\n",
    "- Implement intelligent caching strategies\n",
    "- Use batch processing to optimize throughput\n",
    "- Consider using local models for cost-sensitive applications\n",
    "\n",
    "### üîÆ Advanced Techniques\n",
    "\n",
    "**1. Multimodal Embedding Models**\n",
    "- Use models like CLIP that understand both text and images\n",
    "- Enable direct image-text similarity without separate processing\n",
    "\n",
    "**2. Layout-Aware Document Processing**\n",
    "- Preserve table structures and formatting\n",
    "- Extract and analyze charts, graphs, and diagrams\n",
    "- Understand document hierarchies and sections\n",
    "\n",
    "**3. Real-time Multimodal Processing**\n",
    "- Stream processing for live audio/video\n",
    "- Incremental indexing for new content\n",
    "- Hot/cold storage for different access patterns\n",
    "\n",
    "### üöÄ Production Deployment\n",
    "\n",
    "**Infrastructure:**\n",
    "- Separate processing pipelines for different content types\n",
    "- GPU instances for local vision/audio models\n",
    "- CDN for serving processed content\n",
    "\n",
    "**Monitoring:**\n",
    "- Track processing success rates by content type\n",
    "- Monitor API costs and usage patterns\n",
    "- Alert on processing failures or quality degradation\n",
    "\n",
    "**Scalability:**\n",
    "- Queue-based processing for large content volumes\n",
    "- Distributed vector stores for large-scale deployment\n",
    "- Content deduplication to avoid redundant processing\n",
    "\n",
    "You now have the foundation for building sophisticated multimodal RAG systems! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}