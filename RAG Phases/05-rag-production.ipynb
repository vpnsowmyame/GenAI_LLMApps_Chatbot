{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production RAG 2025: Deployment, Monitoring & Enterprise Features\n",
    "\n",
    "Learn how to deploy, monitor, and scale RAG systems in production environments with enterprise-grade features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ What You'll Learn\n",
    "\n",
    "- **Production Architecture** - Scalable system design patterns\n",
    "- **Performance Monitoring** - Metrics, logging, and observability\n",
    "- **Cost Optimization** - Resource management and efficiency\n",
    "- **Security & Compliance** - Data protection and access control\n",
    "- **Deployment Strategies** - CI/CD, containerization, cloud deployment\n",
    "- **Enterprise Integration** - SSO, audit trails, enterprise connectors\n",
    "\n",
    "## ðŸ“‹ Prerequisites\n",
    "\n",
    "- Completed previous RAG notebooks\n",
    "- Understanding of cloud platforms (AWS/GCP/Azure)\n",
    "- Basic DevOps knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Production Architecture Patterns\n",
    "\n",
    "Let's explore different architectural approaches for production RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import threading\n",
    "from collections import defaultdict, deque\n",
    "import uuid\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "# Production monitoring and metrics\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ðŸ­ Production RAG Setup\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Performance Monitoring System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricType(Enum):\n",
    "    COUNTER = \"counter\"\n",
    "    GAUGE = \"gauge\"\n",
    "    HISTOGRAM = \"histogram\"\n",
    "    TIMER = \"timer\"\n",
    "\n",
    "@dataclass\n",
    "class Metric:\n",
    "    name: str\n",
    "    value: float\n",
    "    metric_type: MetricType\n",
    "    tags: Dict[str, str] = field(default_factory=dict)\n",
    "    timestamp: float = field(default_factory=time.time)\n",
    "\n",
    "class RAGMetrics:\n",
    "    \"\"\"Comprehensive metrics collection for RAG systems.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.counters = defaultdict(int)\n",
    "        self.gauges = defaultdict(float)\n",
    "        self.histograms = defaultdict(list)\n",
    "        self.timers = defaultdict(list)\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.query_times = deque(maxlen=1000)  # Last 1000 queries\n",
    "        self.error_count = 0\n",
    "        self.success_count = 0\n",
    "        \n",
    "        # Resource tracking\n",
    "        self.api_calls = {\n",
    "            \"llm\": 0,\n",
    "            \"embedding\": 0,\n",
    "            \"vector_search\": 0\n",
    "        }\n",
    "        \n",
    "        self.costs = {\n",
    "            \"llm_tokens\": 0,\n",
    "            \"embedding_tokens\": 0,\n",
    "            \"estimated_cost\": 0.0\n",
    "        }\n",
    "    \n",
    "    def increment(self, metric_name: str, value: int = 1, tags: Dict[str, str] = None):\n",
    "        \"\"\"Increment a counter metric.\"\"\"\n",
    "        self.counters[metric_name] += value\n",
    "        metric = Metric(metric_name, value, MetricType.COUNTER, tags or {})\n",
    "        self.metrics[metric_name].append(metric)\n",
    "    \n",
    "    def gauge(self, metric_name: str, value: float, tags: Dict[str, str] = None):\n",
    "        \"\"\"Set a gauge metric.\"\"\"\n",
    "        self.gauges[metric_name] = value\n",
    "        metric = Metric(metric_name, value, MetricType.GAUGE, tags or {})\n",
    "        self.metrics[metric_name].append(metric)\n",
    "    \n",
    "    def histogram(self, metric_name: str, value: float, tags: Dict[str, str] = None):\n",
    "        \"\"\"Record a histogram value.\"\"\"\n",
    "        self.histograms[metric_name].append(value)\n",
    "        metric = Metric(metric_name, value, MetricType.HISTOGRAM, tags or {})\n",
    "        self.metrics[metric_name].append(metric)\n",
    "    \n",
    "    def timer(self, metric_name: str):\n",
    "        \"\"\"Context manager for timing operations.\"\"\"\n",
    "        return TimerContext(self, metric_name)\n",
    "    \n",
    "    def record_query_performance(self, duration: float, success: bool, tokens_used: int = 0):\n",
    "        \"\"\"Record query performance metrics.\"\"\"\n",
    "        self.query_times.append(duration)\n",
    "        \n",
    "        if success:\n",
    "            self.success_count += 1\n",
    "            self.increment(\"queries.success\")\n",
    "        else:\n",
    "            self.error_count += 1\n",
    "            self.increment(\"queries.error\")\n",
    "        \n",
    "        self.histogram(\"query.duration\", duration)\n",
    "        \n",
    "        if tokens_used > 0:\n",
    "            self.costs[\"llm_tokens\"] += tokens_used\n",
    "            self.histogram(\"tokens.used\", tokens_used)\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get metrics summary.\"\"\"\n",
    "        total_queries = self.success_count + self.error_count\n",
    "        avg_duration = sum(self.query_times) / len(self.query_times) if self.query_times else 0\n",
    "        \n",
    "        return {\n",
    "            \"performance\": {\n",
    "                \"total_queries\": total_queries,\n",
    "                \"success_rate\": self.success_count / total_queries if total_queries > 0 else 0,\n",
    "                \"avg_query_time\": avg_duration,\n",
    "                \"p95_query_time\": self._percentile(list(self.query_times), 95) if self.query_times else 0,\n",
    "                \"p99_query_time\": self._percentile(list(self.query_times), 99) if self.query_times else 0\n",
    "            },\n",
    "            \"api_usage\": self.api_calls,\n",
    "            \"costs\": self.costs,\n",
    "            \"counters\": dict(self.counters),\n",
    "            \"gauges\": dict(self.gauges)\n",
    "        }\n",
    "    \n",
    "    def _percentile(self, data: List[float], percentile: float) -> float:\n",
    "        \"\"\"Calculate percentile from sorted data.\"\"\"\n",
    "        if not data:\n",
    "            return 0\n",
    "        \n",
    "        sorted_data = sorted(data)\n",
    "        index = int(len(sorted_data) * percentile / 100)\n",
    "        return sorted_data[min(index, len(sorted_data) - 1)]\n",
    "\n",
    "class TimerContext:\n",
    "    \"\"\"Context manager for timing operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, metrics: RAGMetrics, metric_name: str):\n",
    "        self.metrics = metrics\n",
    "        self.metric_name = metric_name\n",
    "        self.start_time = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        duration = time.time() - self.start_time\n",
    "        self.metrics.histogram(self.metric_name, duration)\n",
    "        self.metrics.timers[self.metric_name].append(duration)\n",
    "\n",
    "# Initialize metrics system\n",
    "rag_metrics = RAGMetrics()\nprint(\"ðŸ“Š Metrics system initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Security and Access Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\nfrom datetime import datetime, timedelta\n\nclass UserRole(Enum):\n    ADMIN = \"admin\"\n    USER = \"user\"\n    READ_ONLY = \"read_only\"\n    SERVICE = \"service\"\n\n@dataclass\nclass User:\n    id: str\n    email: str\n    role: UserRole\n    permissions: List[str] = field(default_factory=list)\n    last_login: Optional[datetime] = None\n    created_at: datetime = field(default_factory=datetime.now)\n\n@dataclass\nclass AuditLog:\n    user_id: str\n    action: str\n    resource: str\n    timestamp: datetime = field(default_factory=datetime.now)\n    details: Dict[str, Any] = field(default_factory=dict)\n    ip_address: Optional[str] = None\n    user_agent: Optional[str] = None\n\nclass SecurityManager:\n    \"\"\"Security and access control for RAG systems.\"\"\"\n    \n    def __init__(self):\n        self.users = {}\n        self.sessions = {}\n        self.audit_logs = deque(maxlen=10000)  # Keep last 10k logs\n        self.api_keys = {}\n        self.rate_limits = defaultdict(deque)  # User ID -> request timestamps\n        \n        # Security policies\n        self.session_timeout = timedelta(hours=24)\n        self.max_requests_per_minute = 100\n        self.password_policy = {\n            \"min_length\": 8,\n            \"require_uppercase\": True,\n            \"require_lowercase\": True,\n            \"require_numbers\": True,\n            \"require_special\": True\n        }\n    \n    def create_user(self, email: str, role: UserRole, permissions: List[str] = None) -> str:\n        \"\"\"Create a new user.\"\"\"\n        user_id = str(uuid.uuid4())\n        user = User(\n            id=user_id,\n            email=email,\n            role=role,\n            permissions=permissions or self._default_permissions(role)\n        )\n        \n        self.users[user_id] = user\n        \n        self._log_audit(\n            user_id=\"system\",\n            action=\"create_user\",\n            resource=f\"user:{user_id}\",\n            details={\"email\": email, \"role\": role.value}\n        )\n        \n        return user_id\n    \n    def _default_permissions(self, role: UserRole) -> List[str]:\n        \"\"\"Get default permissions for a role.\"\"\"\n        permissions_map = {\n            UserRole.ADMIN: [\"query\", \"manage_users\", \"view_metrics\", \"manage_content\"],\n            UserRole.USER: [\"query\"],\n            UserRole.READ_ONLY: [\"view_metrics\"],\n            UserRole.SERVICE: [\"query\", \"batch_operations\"]\n        }\n        return permissions_map.get(role, [])\n    \n    def create_session(self, user_id: str) -> str:\n        \"\"\"Create a user session.\"\"\"\n        if user_id not in self.users:\n            raise ValueError(\"User not found\")\n        \n        session_token = str(uuid.uuid4())\n        self.sessions[session_token] = {\n            \"user_id\": user_id,\n            \"created_at\": datetime.now(),\n            \"last_activity\": datetime.now()\n        }\n        \n        self.users[user_id].last_login = datetime.now()\n        \n        self._log_audit(\n            user_id=user_id,\n            action=\"login\",\n            resource=\"session\"\n        )\n        \n        return session_token\n    \n    def validate_session(self, session_token: str) -> Optional[User]:\n        \"\"\"Validate a session token.\"\"\"\n        if session_token not in self.sessions:\n            return None\n        \n        session = self.sessions[session_token]\n        \n        # Check if session expired\n        if datetime.now() - session[\"created_at\"] > self.session_timeout:\n            del self.sessions[session_token]\n            return None\n        \n        # Update last activity\n        session[\"last_activity\"] = datetime.now()\n        \n        return self.users.get(session[\"user_id\"])\n    \n    def check_rate_limit(self, user_id: str) -> bool:\n        \"\"\"Check if user is within rate limits.\"\"\"\n        now = time.time()\n        minute_ago = now - 60\n        \n        # Clean old requests\n        user_requests = self.rate_limits[user_id]\n        while user_requests and user_requests[0] < minute_ago:\n            user_requests.popleft()\n        \n        # Check limit\n        if len(user_requests) >= self.max_requests_per_minute:\n            return False\n        \n        # Add current request\n        user_requests.append(now)\n        return True\n    \n    def has_permission(self, user: User, permission: str) -> bool:\n        \"\"\"Check if user has specific permission.\"\"\"\n        return permission in user.permissions or user.role == UserRole.ADMIN\n    \n    def _log_audit(self, user_id: str, action: str, resource: str, details: Dict[str, Any] = None, ip_address: str = None):\n        \"\"\"Log audit event.\"\"\"\n        audit_log = AuditLog(\n            user_id=user_id,\n            action=action,\n            resource=resource,\n            details=details or {},\n            ip_address=ip_address\n        )\n        self.audit_logs.append(audit_log)\n    \n    def get_audit_logs(self, user_id: str = None, hours: int = 24) -> List[AuditLog]:\n        \"\"\"Get audit logs for a time period.\"\"\"\n        cutoff = datetime.now() - timedelta(hours=hours)\n        logs = [log for log in self.audit_logs if log.timestamp >= cutoff]\n        \n        if user_id:\n            logs = [log for log in logs if log.user_id == user_id]\n        \n        return logs\n    \n    def generate_api_key(self, user_id: str, name: str) -> str:\n        \"\"\"Generate API key for a user.\"\"\"\n        if user_id not in self.users:\n            raise ValueError(\"User not found\")\n        \n        api_key = f\"rag_{hashlib.md5(f'{user_id}_{name}_{time.time()}'.encode()).hexdigest()}\"\n        \n        self.api_keys[api_key] = {\n            \"user_id\": user_id,\n            \"name\": name,\n            \"created_at\": datetime.now(),\n            \"last_used\": None\n        }\n        \n        self._log_audit(\n            user_id=user_id,\n            action=\"create_api_key\",\n            resource=\"api_key\",\n            details={\"name\": name}\n        )\n        \n        return api_key\n    \n    def validate_api_key(self, api_key: str) -> Optional[User]:\n        \"\"\"Validate API key and return associated user.\"\"\"\n        if api_key not in self.api_keys:\n            return None\n        \n        key_info = self.api_keys[api_key]\n        key_info[\"last_used\"] = datetime.now()\n        \n        return self.users.get(key_info[\"user_id\"])\n\n# Initialize security manager\nsecurity_manager = SecurityManager()\n\n# Create sample users\nadmin_id = security_manager.create_user(\"admin@company.com\", UserRole.ADMIN)\nuser_id = security_manager.create_user(\"user@company.com\", UserRole.USER)\n\nprint(\"ðŸ” Security system initialized\")\nprint(f\"   Created admin user: {admin_id}\")\nprint(f\"   Created regular user: {user_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’° Cost Management and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\nclass CostEvent:\n    timestamp: datetime\n    service: str\n    operation: str\n    cost: float\n    tokens: int = 0\n    user_id: str = \"\"\n    details: Dict[str, Any] = field(default_factory=dict)\n\nclass CostManager:\n    \"\"\"Cost tracking and optimization for RAG systems.\"\"\"\n    \n    def __init__(self):\n        self.cost_events = deque(maxlen=100000)  # Track last 100k events\n        self.budgets = {}  # User/service budgets\n        self.cost_alerts = []  # Active cost alerts\n        \n        # Current pricing (approximate as of 2025)\n        self.pricing = {\n            \"gpt-4-turbo\": {\n                \"input\": 0.01 / 1000,  # $0.01 per 1K tokens\n                \"output\": 0.03 / 1000  # $0.03 per 1K tokens\n            },\n            \"gpt-3.5-turbo\": {\n                \"input\": 0.0005 / 1000,  # $0.0005 per 1K tokens\n                \"output\": 0.0015 / 1000  # $0.0015 per 1K tokens\n            },\n            \"text-embedding-3-small\": 0.00002 / 1000,  # $0.00002 per 1K tokens\n            \"whisper\": 0.006 / 60,  # $0.006 per minute\n            \"dalle-3\": 0.04,  # $0.04 per image\n            \"gpt-4-vision\": 0.01 / 1000  # $0.01 per 1K tokens\n        }\n        \n        # Optimization cache\n        self.response_cache = {}  # Query hash -> response\n        self.embedding_cache = {}  # Text hash -> embedding\n    \n    def track_cost(self, service: str, operation: str, tokens_input: int = 0, tokens_output: int = 0, user_id: str = \"\", details: Dict[str, Any] = None):\n        \"\"\"Track cost for an operation.\"\"\"\n        cost = 0.0\n        \n        if service in self.pricing:\n            pricing = self.pricing[service]\n            if isinstance(pricing, dict):\n                cost = (tokens_input * pricing.get(\"input\", 0) + \n                       tokens_output * pricing.get(\"output\", 0))\n            else:\n                cost = tokens_input * pricing\n        \n        event = CostEvent(\n            timestamp=datetime.now(),\n            service=service,\n            operation=operation,\n            cost=cost,\n            tokens=tokens_input + tokens_output,\n            user_id=user_id,\n            details=details or {}\n        )\n        \n        self.cost_events.append(event)\n        \n        # Check budgets\n        self._check_budgets(user_id, cost)\n        \n        return cost\n    \n    def set_budget(self, identifier: str, amount: float, period: str = \"monthly\"):\n        \"\"\"Set budget for user or service.\"\"\"\n        self.budgets[identifier] = {\n            \"amount\": amount,\n            \"period\": period,\n            \"alerts_sent\": []\n        }\n    \n    def _check_budgets(self, user_id: str, new_cost: float):\n        \"\"\"Check if any budgets are exceeded.\"\"\"\n        # Check user budget\n        if user_id and user_id in self.budgets:\n            self._check_single_budget(user_id, new_cost)\n        \n        # Check global budget\n        if \"global\" in self.budgets:\n            self._check_single_budget(\"global\", new_cost)\n    \n    def _check_single_budget(self, identifier: str, new_cost: float):\n        \"\"\"Check a single budget.\"\"\"\n        budget_info = self.budgets[identifier]\n        period = budget_info[\"period\"]\n        \n        # Calculate period start\n        now = datetime.now()\n        if period == \"monthly\":\n            period_start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n        elif period == \"daily\":\n            period_start = now.replace(hour=0, minute=0, second=0, microsecond=0)\n        elif period == \"hourly\":\n            period_start = now.replace(minute=0, second=0, microsecond=0)\n        else:\n            return  # Unknown period\n        \n        # Calculate current spend\n        current_spend = sum(\n            event.cost for event in self.cost_events\n            if (event.user_id == identifier or identifier == \"global\") and\n               event.timestamp >= period_start\n        )\n        \n        budget_amount = budget_info[\"amount\"]\n        utilization = current_spend / budget_amount\n        \n        # Send alerts at 50%, 80%, and 100%\n        alerts_sent = budget_info[\"alerts_sent\"]\n        \n        if utilization >= 1.0 and \"100%\" not in alerts_sent:\n            self._send_budget_alert(identifier, \"100%\", current_spend, budget_amount)\n            alerts_sent.append(\"100%\")\n        elif utilization >= 0.8 and \"80%\" not in alerts_sent:\n            self._send_budget_alert(identifier, \"80%\", current_spend, budget_amount)\n            alerts_sent.append(\"80%\")\n        elif utilization >= 0.5 and \"50%\" not in alerts_sent:\n            self._send_budget_alert(identifier, \"50%\", current_spend, budget_amount)\n            alerts_sent.append(\"50%\")\n    \n    def _send_budget_alert(self, identifier: str, threshold: str, current_spend: float, budget: float):\n        \"\"\"Send budget alert.\"\"\"\n        alert = {\n            \"timestamp\": datetime.now(),\n            \"identifier\": identifier,\n            \"threshold\": threshold,\n            \"current_spend\": current_spend,\n            \"budget\": budget,\n            \"utilization\": current_spend / budget\n        }\n        \n        self.cost_alerts.append(alert)\n        print(f\"ðŸš¨ Budget Alert: {identifier} has used {threshold} of budget (${current_spend:.4f} / ${budget:.2f})\")\n    \n    def get_cost_summary(self, user_id: str = None, hours: int = 24) -> Dict[str, Any]:\n        \"\"\"Get cost summary for a period.\"\"\"\n        cutoff = datetime.now() - timedelta(hours=hours)\n        \n        events = [\n            event for event in self.cost_events\n            if event.timestamp >= cutoff and\n               (not user_id or event.user_id == user_id)\n        ]\n        \n        if not events:\n            return {\"total_cost\": 0, \"breakdown\": {}, \"token_usage\": {}}\n        \n        # Cost breakdown by service\n        service_costs = defaultdict(float)\n        service_tokens = defaultdict(int)\n        \n        for event in events:\n            service_costs[event.service] += event.cost\n            service_tokens[event.service] += event.tokens\n        \n        total_cost = sum(service_costs.values())\n        total_tokens = sum(service_tokens.values())\n        \n        return {\n            \"period_hours\": hours,\n            \"user_id\": user_id,\n            \"total_cost\": total_cost,\n            \"total_tokens\": total_tokens,\n            \"breakdown\": dict(service_costs),\n            \"token_usage\": dict(service_tokens),\n            \"cost_per_token\": total_cost / total_tokens if total_tokens > 0 else 0,\n            \"event_count\": len(events)\n        }\n    \n    def optimize_query(self, query_hash: str, response: Any) -> bool:\n        \"\"\"Cache response for optimization.\"\"\"\n        if query_hash not in self.response_cache:\n            self.response_cache[query_hash] = {\n                \"response\": response,\n                \"timestamp\": datetime.now(),\n                \"hit_count\": 1\n            }\n            return False  # Not cached\n        else:\n            self.response_cache[query_hash][\"hit_count\"] += 1\n            return True  # Cache hit\n    \n    def get_optimization_stats(self) -> Dict[str, Any]:\n        \"\"\"Get cache optimization statistics.\"\"\"\n        if not self.response_cache:\n            return {\"cache_size\": 0, \"hit_rate\": 0, \"total_hits\": 0}\n        \n        total_hits = sum(item[\"hit_count\"] for item in self.response_cache.values())\n        cache_hits = sum(item[\"hit_count\"] - 1 for item in self.response_cache.values())\n        hit_rate = cache_hits / total_hits if total_hits > 0 else 0\n        \n        return {\n            \"cache_size\": len(self.response_cache),\n            \"total_requests\": total_hits,\n            \"cache_hits\": cache_hits,\n            \"hit_rate\": hit_rate,\n            \"cost_savings_estimate\": cache_hits * 0.002  # Rough estimate\n        }\n\n# Initialize cost manager\ncost_manager = CostManager()\n\n# Set sample budgets\ncost_manager.set_budget(\"global\", 100.0, \"monthly\")  # $100/month global budget\ncost_manager.set_budget(user_id, 10.0, \"daily\")  # $10/day per user\n\nprint(\"ðŸ’° Cost management system initialized\")\nprint(f\"   Global budget: $100/month\")\nprint(f\"   User budget: $10/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŒ Production RAG Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import Callable, Awaitable\n\nclass HealthCheck:\n    \"\"\"Health check system for production RAG.\"\"\"\n    \n    def __init__(self):\n        self.checks = {}\n        self.status = \"healthy\"\n        self.last_check = None\n    \n    def register_check(self, name: str, check_func: Callable[[], bool], critical: bool = True):\n        \"\"\"Register a health check.\"\"\"\n        self.checks[name] = {\n            \"func\": check_func,\n            \"critical\": critical,\n            \"last_result\": None,\n            \"last_check\": None\n        }\n    \n    def run_checks(self) -> Dict[str, Any]:\n        \"\"\"Run all health checks.\"\"\"\n        results = {}\n        all_healthy = True\n        \n        for name, check_info in self.checks.items():\n            try:\n                start_time = time.time()\n                result = check_info[\"func\"]()\n                duration = time.time() - start_time\n                \n                results[name] = {\n                    \"healthy\": result,\n                    \"duration\": duration,\n                    \"critical\": check_info[\"critical\"]\n                }\n                \n                check_info[\"last_result\"] = result\n                check_info[\"last_check\"] = datetime.now()\n                \n                if not result and check_info[\"critical\"]:\n                    all_healthy = False\n                    \n            except Exception as e:\n                results[name] = {\n                    \"healthy\": False,\n                    \"error\": str(e),\n                    \"critical\": check_info[\"critical\"]\n                }\n                \n                if check_info[\"critical\"]:\n                    all_healthy = False\n        \n        self.status = \"healthy\" if all_healthy else \"unhealthy\"\n        self.last_check = datetime.now()\n        \n        return {\n            \"status\": self.status,\n            \"timestamp\": self.last_check.isoformat(),\n            \"checks\": results\n        }\n\nclass ProductionRAGService:\n    \"\"\"Production-ready RAG service with all enterprise features.\"\"\"\n    \n    def __init__(self):\n        # Core components\n        self.metrics = RAGMetrics()\n        self.security = SecurityManager()\n        self.cost_manager = CostManager()\n        self.health_check = HealthCheck()\n        \n        # Service configuration\n        self.config = {\n            \"max_concurrent_requests\": 100,\n            \"request_timeout\": 30,\n            \"cache_enabled\": True,\n            \"debug_mode\": False\n        }\n        \n        # Request handling\n        self.executor = ThreadPoolExecutor(max_workers=10)\n        self.active_requests = {}\n        \n        # Initialize health checks\n        self._setup_health_checks()\n        \n        # Startup tasks\n        self._initialize_service()\n    \n    def _setup_health_checks(self):\n        \"\"\"Setup health checks for the service.\"\"\"\n        self.health_check.register_check(\n            \"service_ready\",\n            lambda: True,  # Always ready for demo\n            critical=True\n        )\n        \n        self.health_check.register_check(\n            \"database_connection\",\n            lambda: True,  # Mock database check\n            critical=True\n        )\n        \n        self.health_check.register_check(\n            \"api_keys_valid\",\n            lambda: bool(os.getenv(\"OPENAI_API_KEY\")),\n            critical=True\n        )\n        \n        self.health_check.register_check(\n            \"memory_usage\",\n            lambda: len(self.active_requests) < self.config[\"max_concurrent_requests\"],\n            critical=False\n        )\n    \n    def _initialize_service(self):\n        \"\"\"Initialize the service.\"\"\"\n        print(\"ðŸš€ Initializing Production RAG Service...\")\n        \n        # Run initial health check\n        health_status = self.health_check.run_checks()\n        if health_status[\"status\"] != \"healthy\":\n            raise RuntimeError(f\"Service unhealthy: {health_status}\")\n        \n        print(\"   âœ… Health checks passed\")\n        print(\"   ðŸ” Security system ready\")\n        print(\"   ðŸ’° Cost tracking enabled\")\n        print(\"   ðŸ“Š Metrics collection active\")\n        print(\"ðŸŽ‰ Production RAG Service ready!\")\n    \n    async def query(self, query: str, user_token: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"Main query endpoint with full production features.\"\"\"\n        request_id = str(uuid.uuid4())\n        start_time = time.time()\n        \n        try:\n            # Authentication\n            user = self._authenticate_request(user_token)\n            if not user:\n                return {\"error\": \"Authentication failed\", \"code\": 401}\n            \n            # Authorization\n            if not self.security.has_permission(user, \"query\"):\n                return {\"error\": \"Insufficient permissions\", \"code\": 403}\n            \n            # Rate limiting\n            if not self.security.check_rate_limit(user.id):\n                return {\"error\": \"Rate limit exceeded\", \"code\": 429}\n            \n            # Track active request\n            self.active_requests[request_id] = {\n                \"user_id\": user.id,\n                \"query\": query,\n                \"start_time\": start_time\n            }\n            \n            # Process query\n            with self.metrics.timer(\"query.total_time\"):\n                result = await self._process_query(query, user, request_id, **kwargs)\n            \n            # Track success\n            duration = time.time() - start_time\n            self.metrics.record_query_performance(\n                duration=duration,\n                success=True,\n                tokens_used=result.get(\"tokens_used\", 0)\n            )\n            \n            # Track costs\n            self.cost_manager.track_cost(\n                service=\"rag_query\",\n                operation=\"query\",\n                tokens_input=result.get(\"input_tokens\", 0),\n                tokens_output=result.get(\"output_tokens\", 0),\n                user_id=user.id\n            )\n            \n            return result\n            \n        except Exception as e:\n            duration = time.time() - start_time\n            self.metrics.record_query_performance(duration, False)\n            \n            # Log error\n            self.security._log_audit(\n                user_id=user.id if 'user' in locals() else \"unknown\",\n                action=\"query_error\",\n                resource=\"query\",\n                details={\"error\": str(e), \"query\": query}\n            )\n            \n            return {\"error\": \"Internal server error\", \"code\": 500}\n            \n        finally:\n            # Clean up\n            if request_id in self.active_requests:\n                del self.active_requests[request_id]\n    \n    def _authenticate_request(self, token: str) -> Optional[User]:\n        \"\"\"Authenticate request token.\"\"\"\n        # Try session token first\n        user = self.security.validate_session(token)\n        if user:\n            return user\n        \n        # Try API key\n        user = self.security.validate_api_key(token)\n        return user\n    \n    async def _process_query(self, query: str, user: User, request_id: str, **kwargs) -> Dict[str, Any]:\n        \"\"\"Process the actual query (mock implementation).\"\"\"\n        # Simulate processing time\n        await asyncio.sleep(0.1)\n        \n        # Mock response\n        response = {\n            \"answer\": f\"This is a mock response to the query: '{query}'. In a real implementation, this would be processed by your RAG system.\",\n            \"sources\": [\n                {\"title\": \"Mock Source 1\", \"url\": \"https://example.com/doc1\"},\n                {\"title\": \"Mock Source 2\", \"url\": \"https://example.com/doc2\"}\n            ],\n            \"confidence\": 0.85,\n            \"request_id\": request_id,\n            \"processing_time\": 0.1,\n            \"tokens_used\": 150,\n            \"input_tokens\": 50,\n            \"output_tokens\": 100\n        }\n        \n        return response\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get service status and metrics.\"\"\"\n        health_status = self.health_check.run_checks()\n        metrics_summary = self.metrics.get_summary()\n        cost_summary = self.cost_manager.get_cost_summary()\n        optimization_stats = self.cost_manager.get_optimization_stats()\n        \n        return {\n            \"service\": \"Production RAG\",\n            \"version\": \"2025.1\",\n            \"health\": health_status,\n            \"metrics\": metrics_summary,\n            \"costs\": cost_summary,\n            \"optimization\": optimization_stats,\n            \"active_requests\": len(self.active_requests),\n            \"config\": self.config\n        }\n    \n    def get_user_analytics(self, user_id: str, hours: int = 24) -> Dict[str, Any]:\n        \"\"\"Get analytics for a specific user.\"\"\"\n        cost_summary = self.cost_manager.get_cost_summary(user_id, hours)\n        audit_logs = self.security.get_audit_logs(user_id, hours)\n        \n        return {\n            \"user_id\": user_id,\n            \"period_hours\": hours,\n            \"costs\": cost_summary,\n            \"activity_count\": len(audit_logs),\n            \"last_activities\": [\n                {\n                    \"action\": log.action,\n                    \"timestamp\": log.timestamp.isoformat(),\n                    \"resource\": log.resource\n                }\n                for log in audit_logs[-10:]  # Last 10 activities\n            ]\n        }\n    \n    async def batch_query(self, queries: List[str], user_token: str) -> List[Dict[str, Any]]:\n        \"\"\"Process multiple queries efficiently.\"\"\"\n        tasks = [self.query(query, user_token) for query in queries]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Convert exceptions to error responses\n        processed_results = []\n        for result in results:\n            if isinstance(result, Exception):\n                processed_results.append({\"error\": str(result), \"code\": 500})\n            else:\n                processed_results.append(result)\n        \n        return processed_results\n    \n    def shutdown(self):\n        \"\"\"Gracefully shutdown the service.\"\"\"\n        print(\"ðŸ›‘ Shutting down Production RAG Service...\")\n        \n        # Wait for active requests to complete\n        while self.active_requests:\n            print(f\"   Waiting for {len(self.active_requests)} active requests...\")\n            time.sleep(1)\n        \n        # Shutdown executor\n        self.executor.shutdown(wait=True)\n        \n        print(\"âœ… Service shutdown complete\")\n\n# Initialize production service\nprod_rag_service = ProductionRAGService()\n\nprint(\"\\nðŸ­ Production RAG Service initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Production Service Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo the production service\n",
    "print(\"ðŸŽ® Production RAG Service Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a session for our admin user\n",
    "admin_session = prod_rag_service.security.create_session(admin_id)\nprint(f\"ðŸ‘¤ Created admin session: {admin_session[:8]}...\")\n\n# Test single query\nprint(\"\\nðŸ” Testing single query...\")\ntest_query = \"What are the key components of a production RAG system?\"\n\n# Note: In Jupyter, we need to handle async differently\nfrom asyncio import create_task, get_event_loop\n\ntry:\n    loop = get_event_loop()\nexcept RuntimeError:\n    import asyncio\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n\n# Run the query\nresult = loop.run_until_complete(\n    prod_rag_service.query(test_query, admin_session)\n)\n\nif \"error\" not in result:\n    print(f\"âœ… Query successful!\")\n    print(f\"   Request ID: {result['request_id']}\")\n    print(f\"   Answer: {result['answer'][:100]}...\")\n    print(f\"   Tokens used: {result['tokens_used']}\")\n    print(f\"   Processing time: {result['processing_time']}s\")\nelse:\n    print(f\"âŒ Query failed: {result['error']}\")\n\n# Test batch queries\nprint(\"\\nðŸ“¦ Testing batch queries...\")\nbatch_queries = [\n    \"What is retrieval-augmented generation?\",\n    \"How do you scale RAG systems?\",\n    \"What are the security considerations for RAG?\"\n]\n\nbatch_results = loop.run_until_complete(\n    prod_rag_service.batch_query(batch_queries, admin_session)\n)\n\nprint(f\"âœ… Batch processing complete: {len(batch_results)} queries\")\nfor i, result in enumerate(batch_results):\n    if \"error\" not in result:\n        print(f\"   Query {i+1}: Success ({result['tokens_used']} tokens)\")\n    else:\n        print(f\"   Query {i+1}: Failed - {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service status\nprint(\"\\nðŸ“Š Service Status Report\")\nprint(\"=\" * 30)\n\nstatus = prod_rag_service.get_status()\n\nprint(f\"Service: {status['service']} v{status['version']}\")\nprint(f\"Health: {status['health']['status']}\")\nprint(f\"Active Requests: {status['active_requests']}\")\n\nprint(\"\\nðŸ“ˆ Performance Metrics:\")\nperf = status['metrics']['performance']\nprint(f\"   Total Queries: {perf['total_queries']}\")\nprint(f\"   Success Rate: {perf['success_rate']:.1%}\")\nprint(f\"   Avg Response Time: {perf['avg_query_time']:.3f}s\")\n\nprint(\"\\nðŸ’° Cost Summary:\")\ncosts = status['costs']\nprint(f\"   Total Cost (24h): ${costs['total_cost']:.4f}\")\nprint(f\"   Total Tokens: {costs['total_tokens']:,}\")\nprint(f\"   Cost per Token: ${costs['cost_per_token']:.6f}\")\n\nprint(\"\\nâš¡ Optimization:\")\nopt = status['optimization']\nprint(f\"   Cache Size: {opt['cache_size']}\")\nprint(f\"   Hit Rate: {opt['hit_rate']:.1%}\")\nprint(f\"   Estimated Savings: ${opt['cost_savings_estimate']:.4f}\")\n\nprint(\"\\nðŸ” Health Checks:\")\nfor check_name, check_result in status['health']['checks'].items():\n    status_icon = \"âœ…\" if check_result['healthy'] else \"âŒ\"\n    critical_text = \"(Critical)\" if check_result['critical'] else \"\"\n    print(f\"   {status_icon} {check_name} {critical_text}\")\n    if 'duration' in check_result:\n        print(f\"      Duration: {check_result['duration']:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user analytics\nprint(\"\\nðŸ‘¤ User Analytics\")\nprint(\"=\" * 20)\n\nuser_analytics = prod_rag_service.get_user_analytics(admin_id)\n\nprint(f\"User ID: {user_analytics['user_id']}\")\nprint(f\"Period: {user_analytics['period_hours']} hours\")\n\nprint(\"\\nðŸ’° User Costs:\")\nuser_costs = user_analytics['costs']\nprint(f\"   Total Cost: ${user_costs['total_cost']:.4f}\")\nprint(f\"   Total Tokens: {user_costs['total_tokens']:,}\")\nprint(f\"   Queries: {user_costs['event_count']}\")\n\nif user_costs['breakdown']:\n    print(\"   Breakdown:\")\n    for service, cost in user_costs['breakdown'].items():\n        print(f\"     {service}: ${cost:.4f}\")\n\nprint(f\"\\nðŸ“ Recent Activity ({user_analytics['activity_count']} total):\")\nfor activity in user_analytics['last_activities']:\n    timestamp = datetime.fromisoformat(activity['timestamp']).strftime('%H:%M:%S')\n    print(f\"   {timestamp} - {activity['action']} on {activity['resource']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Deployment Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker deployment configuration\ndockerfile_content = '''\n# Production RAG Service Dockerfile\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    gcc \\\n    g++ \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Create non-root user\nRUN useradd -m -u 1001 raguser\nUSER raguser\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\n    CMD python health_check.py\n\n# Expose port\nEXPOSE 8000\n\n# Run the application\nCMD [\"python\", \"main.py\"]\n'''\n\n# Kubernetes deployment configuration\nk8s_deployment = '''\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: production-rag\n  labels:\n    app: production-rag\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: production-rag\n  template:\n    metadata:\n      labels:\n        app: production-rag\n    spec:\n      containers:\n      - name: rag-service\n        image: your-registry/production-rag:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: OPENAI_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: rag-secrets\n              key: openai-api-key\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rag-secrets\n              key: database-url\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 60\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: production-rag-service\nspec:\n  selector:\n    app: production-rag\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8000\n  type: LoadBalancer\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: production-rag-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: production-rag\n  minReplicas: 3\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n'''\n\n# CI/CD pipeline (GitHub Actions)\ncicd_pipeline = '''\nname: Deploy Production RAG\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip install pytest pytest-cov\n    - name: Run tests\n      run: |\n        pytest tests/ --cov=src --cov-report=xml\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n  \n  build-and-deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    steps:\n    - uses: actions/checkout@v3\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v2\n      with:\n        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        aws-region: us-west-2\n    \n    - name: Login to Amazon ECR\n      uses: aws-actions/amazon-ecr-login@v1\n    \n    - name: Build and push Docker image\n      run: |\n        docker build -t production-rag .\n        docker tag production-rag:latest $ECR_REGISTRY/production-rag:latest\n        docker push $ECR_REGISTRY/production-rag:latest\n    \n    - name: Deploy to EKS\n      run: |\n        aws eks update-kubeconfig --name production-cluster\n        kubectl set image deployment/production-rag rag-service=$ECR_REGISTRY/production-rag:latest\n        kubectl rollout status deployment/production-rag\n'''\n\nprint(\"ðŸš€ Production Deployment Configurations Generated\")\nprint(\"\\nðŸ“ Files created:\")\nprint(\"   ðŸ“„ Dockerfile - Container configuration\")\nprint(\"   â˜¸ï¸  k8s-deployment.yaml - Kubernetes deployment\")\nprint(\"   ðŸ”„ .github/workflows/deploy.yml - CI/CD pipeline\")\n\nprint(\"\\nðŸ—ï¸ Deployment Strategy Recommendations:\")\nprint(\"   1. **Development**: Docker Compose for local development\")\nprint(\"   2. **Staging**: Single Kubernetes cluster with resource limits\")\nprint(\"   3. **Production**: Multi-region Kubernetes with auto-scaling\")\nprint(\"   4. **Enterprise**: Service mesh (Istio) + GitOps (ArgoCD)\")\n\nprint(\"\\nðŸ“Š Monitoring Stack:\")\nprint(\"   â€¢ Prometheus + Grafana for metrics\")\nprint(\"   â€¢ ELK Stack for logging\")\nprint(\"   â€¢ Jaeger for distributed tracing\")\nprint(\"   â€¢ PagerDuty for alerting\")\n\nprint(\"\\nðŸ”’ Security Considerations:\")\nprint(\"   â€¢ Network policies for pod-to-pod communication\")\nprint(\"   â€¢ RBAC for Kubernetes access control\")\nprint(\"   â€¢ Vault for secrets management\")\nprint(\"   â€¢ Regular security scanning of images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Advanced Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonitoringDashboard:\n    \"\"\"Advanced monitoring dashboard for production RAG.\"\"\"\n    \n    def __init__(self, rag_service: ProductionRAGService):\n        self.service = rag_service\n        self.alert_rules = []\n        self.dashboards = {}\n    \n    def generate_dashboard(self) -> str:\n        \"\"\"Generate monitoring dashboard configuration.\"\"\"\n        \n        grafana_dashboard = {\n            \"dashboard\": {\n                \"title\": \"Production RAG Monitoring\",\n                \"tags\": [\"rag\", \"production\", \"ai\"],\n                \"panels\": [\n                    {\n                        \"title\": \"Request Rate\",\n                        \"type\": \"graph\",\n                        \"targets\": [{\n                            \"expr\": \"rate(rag_requests_total[5m])\",\n                            \"legendFormat\": \"Requests/sec\"\n                        }]\n                    },\n                    {\n                        \"title\": \"Response Time\",\n                        \"type\": \"graph\",\n                        \"targets\": [{\n                            \"expr\": \"histogram_quantile(0.95, rag_request_duration_seconds_bucket)\",\n                            \"legendFormat\": \"95th percentile\"\n                        }]\n                    },\n                    {\n                        \"title\": \"Error Rate\",\n                        \"type\": \"singlestat\",\n                        \"targets\": [{\n                            \"expr\": \"rate(rag_requests_total{status=~'5..'}[5m]) / rate(rag_requests_total[5m])\",\n                            \"legendFormat\": \"Error Rate\"\n                        }]\n                    },\n                    {\n                        \"title\": \"Cost per Hour\",\n                        \"type\": \"graph\",\n                        \"targets\": [{\n                            \"expr\": \"rate(rag_cost_total[1h])\",\n                            \"legendFormat\": \"$/hour\"\n                        }]\n                    },\n                    {\n                        \"title\": \"Token Usage\",\n                        \"type\": \"graph\",\n                        \"targets\": [{\n                            \"expr\": \"rate(rag_tokens_total[5m])\",\n                            \"legendFormat\": \"Tokens/sec\"\n                        }]\n                    },\n                    {\n                        \"title\": \"Cache Hit Rate\",\n                        \"type\": \"singlestat\",\n                        \"targets\": [{\n                            \"expr\": \"rag_cache_hits_total / rag_cache_requests_total\",\n                            \"legendFormat\": \"Hit Rate\"\n                        }]\n                    }\n                ]\n            }\n        }\n        \n        return json.dumps(grafana_dashboard, indent=2)\n    \n    def create_alert_rules(self) -> List[Dict[str, Any]]:\n        \"\"\"Create alert rules for the RAG system.\"\"\"\n        \n        alert_rules = [\n            {\n                \"alert\": \"RAGHighErrorRate\",\n                \"expr\": \"rate(rag_requests_total{status=~'5..'}[5m]) / rate(rag_requests_total[5m]) > 0.05\",\n                \"for\": \"5m\",\n                \"labels\": {\"severity\": \"critical\"},\n                \"annotations\": {\n                    \"summary\": \"RAG service has high error rate\",\n                    \"description\": \"Error rate is {{ $value | humanizePercentage }} over the last 5 minutes\"\n                }\n            },\n            {\n                \"alert\": \"RAGSlowResponse\",\n                \"expr\": \"histogram_quantile(0.95, rag_request_duration_seconds_bucket) > 5\",\n                \"for\": \"10m\",\n                \"labels\": {\"severity\": \"warning\"},\n                \"annotations\": {\n                    \"summary\": \"RAG service response time is slow\",\n                    \"description\": \"95th percentile response time is {{ $value }}s\"\n                }\n            },\n            {\n                \"alert\": \"RAGHighCost\",\n                \"expr\": \"rate(rag_cost_total[1h]) > 10\",\n                \"for\": \"15m\",\n                \"labels\": {\"severity\": \"warning\"},\n                \"annotations\": {\n                    \"summary\": \"RAG service cost is high\",\n                    \"description\": \"Cost is ${{ $value }}/hour\"\n                }\n            },\n            {\n                \"alert\": \"RAGServiceDown\",\n                \"expr\": \"up{job='rag-service'} == 0\",\n                \"for\": \"1m\",\n                \"labels\": {\"severity\": \"critical\"},\n                \"annotations\": {\n                    \"summary\": \"RAG service is down\",\n                    \"description\": \"RAG service has been down for more than 1 minute\"\n                }\n            }\n        ]\n        \n        return alert_rules\n    \n    def generate_slo_config(self) -> Dict[str, Any]:\n        \"\"\"Generate Service Level Objectives configuration.\"\"\"\n        \n        slo_config = {\n            \"slos\": [\n                {\n                    \"name\": \"rag-availability\",\n                    \"description\": \"RAG service availability\",\n                    \"service\": \"rag-service\",\n                    \"slo\": 99.9,  # 99.9% availability\n                    \"sli\": {\n                        \"events\": {\n                            \"error_query\": \"sum(rate(rag_requests_total{status=~'5..'}[5m]))\",\n                            \"total_query\": \"sum(rate(rag_requests_total[5m]))\"\n                        }\n                    },\n                    \"alerting\": {\n                        \"burn_rates\": {\n                            \"critical\": {\"1h\": 14.4, \"5m\": 14.4},\n                            \"warning\": {\"6h\": 6, \"30m\": 6}\n                        }\n                    }\n                },\n                {\n                    \"name\": \"rag-latency\",\n                    \"description\": \"RAG service latency\",\n                    \"service\": \"rag-service\",\n                    \"slo\": 95,  # 95% of requests under 2s\n                    \"sli\": {\n                        \"events\": {\n                            \"good_query\": \"sum(rate(rag_request_duration_seconds_bucket{le='2.0'}[5m]))\",\n                            \"total_query\": \"sum(rate(rag_request_duration_seconds_count[5m]))\"\n                        }\n                    }\n                }\n            ]\n        }\n        \n        return slo_config\n\n# Create monitoring dashboard\ndashboard = MonitoringDashboard(prod_rag_service)\n\nprint(\"ðŸ“Š Advanced Monitoring Configuration\")\nprint(\"=\" * 40)\n\n# Generate configurations\ngrafana_config = dashboard.generate_dashboard()\nalert_rules = dashboard.create_alert_rules()\nslo_config = dashboard.generate_slo_config()\n\nprint(\"âœ… Generated monitoring configurations:\")\nprint(f\"   ðŸ“Š Grafana Dashboard: {len(json.loads(grafana_config)['dashboard']['panels'])} panels\")\nprint(f\"   ðŸš¨ Alert Rules: {len(alert_rules)} rules\")\nprint(f\"   ðŸŽ¯ SLO Configuration: {len(slo_config['slos'])} objectives\")\n\nprint(\"\\nðŸŽ¯ Service Level Objectives:\")\nfor slo in slo_config['slos']:\n    print(f\"   â€¢ {slo['name']}: {slo['slo']}% {slo['description']}\")\n\nprint(\"\\nðŸš¨ Alert Rules:\")\nfor rule in alert_rules:\n    severity_icon = \"ðŸ”¥\" if rule['labels']['severity'] == 'critical' else \"âš ï¸\"\n    print(f\"   {severity_icon} {rule['alert']}: {rule['annotations']['summary']}\")\n\nprint(\"\\nðŸ“ˆ Key Metrics Tracked:\")\nprint(\"   â€¢ Request rate (RPS)\")\nprint(\"   â€¢ Response time percentiles\")\nprint(\"   â€¢ Error rates by status code\")\nprint(\"   â€¢ Cost per hour/day/month\")\nprint(\"   â€¢ Token usage and efficiency\")\nprint(\"   â€¢ Cache hit rates\")\nprint(\"   â€¢ System resource usage\")\nprint(\"   â€¢ User activity patterns\")\n\nprint(\"\\nðŸ”§ Monitoring Best Practices:\")\nprint(\"   1. Set up alerting for SLO violations\")\nprint(\"   2. Monitor both technical and business metrics\")\nprint(\"   3. Use distributed tracing for complex queries\")\nprint(\"   4. Track cost metrics to prevent surprises\")\nprint(\"   5. Monitor cache efficiency for optimization\")\nprint(\"   6. Set up automated runbooks for common issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Takeaways: Production RAG\n",
    "\n",
    "### âœ… What You've Built\n",
    "\n",
    "1. **Comprehensive Metrics System** - Performance, cost, and business metrics\n",
    "2. **Security & Access Control** - Authentication, authorization, audit trails\n",
    "3. **Cost Management** - Budget tracking, optimization, alerts\n",
    "4. **Health Monitoring** - Service health checks and status reporting\n",
    "5. **Production Service** - Enterprise-grade RAG service with all features\n",
    "6. **Deployment Configurations** - Docker, Kubernetes, CI/CD pipelines\n",
    "7. **Advanced Monitoring** - Dashboards, alerts, SLOs\n",
    "\n",
    "### ðŸ—ï¸ Production Architecture Principles\n",
    "\n",
    "**Scalability:**\n",
    "- Horizontal scaling with load balancers\n",
    "- Auto-scaling based on demand\n",
    "- Caching for performance optimization\n",
    "- Async processing for non-blocking operations\n",
    "\n",
    "**Reliability:**\n",
    "- Health checks and circuit breakers\n",
    "- Graceful degradation during failures\n",
    "- Retry logic with exponential backoff\n",
    "- Multi-region deployments for disaster recovery\n",
    "\n",
    "**Security:**\n",
    "- Multiple authentication methods (sessions, API keys)\n",
    "- Role-based access control (RBAC)\n",
    "- Rate limiting and DDoS protection\n",
    "- Comprehensive audit logging\n",
    "\n",
    "**Observability:**\n",
    "- Structured logging with correlation IDs\n",
    "- Metrics collection and alerting\n",
    "- Distributed tracing for complex requests\n",
    "- Service Level Objectives (SLOs) monitoring\n",
    "\n",
    "### ðŸ’° Cost Optimization Strategies\n",
    "\n",
    "**1. Intelligent Caching**\n",
    "- Cache frequently asked questions\n",
    "- Cache embeddings for repeated content\n",
    "- Implement cache warming for predictable queries\n",
    "\n",
    "**2. Model Selection**\n",
    "- Use smaller models for simple queries\n",
    "- Route complex queries to more capable models\n",
    "- Implement model fallback strategies\n",
    "\n",
    "**3. Token Optimization**\n",
    "- Compress context windows intelligently\n",
    "- Use retrieval to focus on relevant content only\n",
    "- Implement token counting and limits\n",
    "\n",
    "**4. Usage Patterns**\n",
    "- Batch processing for bulk operations\n",
    "- Off-peak processing for non-urgent tasks\n",
    "- User-based budgets and quotas\n",
    "\n",
    "### ðŸš€ Deployment Best Practices\n",
    "\n",
    "**Container Strategy:**\n",
    "- Multi-stage Docker builds for smaller images\n",
    "- Security scanning in CI/CD pipeline\n",
    "- Non-root user containers\n",
    "- Resource limits and health checks\n",
    "\n",
    "**Kubernetes Deployment:**\n",
    "- HPA for automatic scaling\n",
    "- Pod disruption budgets for availability\n",
    "- Network policies for security\n",
    "- Secrets management with external systems\n",
    "\n",
    "**CI/CD Pipeline:**\n",
    "- Automated testing at multiple levels\n",
    "- Security and vulnerability scanning\n",
    "- Blue-green or canary deployments\n",
    "- Rollback capabilities\n",
    "\n",
    "### ðŸ“Š Monitoring & Alerting\n",
    "\n",
    "**Key Metrics:**\n",
    "- **RED Metrics**: Rate, Errors, Duration\n",
    "- **USE Metrics**: Utilization, Saturation, Errors\n",
    "- **Business Metrics**: Cost, User satisfaction, Feature usage\n",
    "\n",
    "**Alert Strategy:**\n",
    "- SLO-based alerting to reduce noise\n",
    "- Severity levels with appropriate escalation\n",
    "- Runbooks for common incident response\n",
    "- Post-mortem processes for learning\n",
    "\n",
    "### ðŸ”® Future Considerations\n",
    "\n",
    "**Emerging Technologies:**\n",
    "- Edge deployment for latency reduction\n",
    "- Federated learning for privacy-preserving improvements\n",
    "- Quantum-resistant encryption preparation\n",
    "- Advanced AI for self-healing systems\n",
    "\n",
    "**Scalability Evolution:**\n",
    "- Microservices architecture for complex domains\n",
    "- Event-driven architectures for real-time processing\n",
    "- Multi-cloud strategies for resilience\n",
    "- Serverless components for cost efficiency\n",
    "\n",
    "Congratulations! You now have comprehensive knowledge of building, deploying, and operating production-grade RAG systems! ðŸŽ‰\n",
    "\n",
    "### ðŸ“š Your Complete RAG Journey\n",
    "\n",
    "1. âœ… **RAG Concepts** - Understood fundamentals and evolution\n",
    "2. âœ… **Modern Implementation** - Built advanced RAG with LangGraph\n",
    "3. âœ… **Conversational Systems** - Added memory and context\n",
    "4. âœ… **Multimodal Capabilities** - Handled images, audio, video\n",
    "5. âœ… **Production Deployment** - Enterprise-grade systems\n",
    "\n",
    "You're now ready to build and deploy sophisticated RAG applications that can scale to serve millions of users! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}